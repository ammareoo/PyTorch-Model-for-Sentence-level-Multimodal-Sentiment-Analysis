{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fe3bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting d2l\n",
      "  Downloading d2l-0.17.6-py3-none-any.whl (112 kB)\n",
      "     ------------------------------------ 112.6/112.6 kB 155.9 kB/s eta 0:00:00\n",
      "Collecting pandas==1.2.4\n",
      "  Downloading pandas-1.2.4-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "     ---------------------------------------- 9.3/9.3 MB 363.2 kB/s eta 0:00:00\n",
      "Collecting matplotlib==3.5.1\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 264.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy==1.21.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from d2l) (1.21.5)\n",
      "Collecting requests==2.25.1\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "     -------------------------------------- 61.2/61.2 kB 326.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jupyter==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from d2l) (1.0.0)\n",
      "Requirement already satisfied: qtconsole in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (5.2.2)\n",
      "Requirement already satisfied: notebook in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.4.12)\n",
      "Requirement already satisfied: ipykernel in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.15.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (7.6.5)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.4.4)\n",
      "Requirement already satisfied: jupyter-console in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==1.2.4->d2l) (2022.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (4.0.0)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.8/58.8 kB 258.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (1.26.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.16.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.1.6)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (23.2.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (7.31.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.5.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (7.3.4)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.5.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.5.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.20)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.11.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.11.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.5.13)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.1)\n",
      "Requirement already satisfied: testpath in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (0.13.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (0.14.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
      "Requirement already satisfied: qtpy in c:\\programdata\\anaconda3\\lib\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (63.4.1)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2>=2.4->nbconvert->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->nbconvert->jupyter==1.0.0->d2l) (302)\n",
      "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.16.0)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Installing collected packages: idna, requests, pandas, matplotlib, d2l\n",
      "Successfully installed d2l-0.17.6 idna-2.10 matplotlib-3.5.1 pandas-1.2.4 requests-2.25.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.25.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (9.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pytesseract.exe is installed in 'C:\\Users\\La\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (9.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (4.29.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 243.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from timm) (0.15.2)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-win_amd64.whl (263 kB)\n",
      "     ------------------------------------ 263.9/263.9 kB 405.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (2022.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub->timm) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (21.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (1.26.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub->timm) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.2.1)\n",
      "Installing collected packages: safetensors, timm\n",
      "Successfully installed safetensors-0.3.1 timm-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install d2l\n",
    "!pip3 install pillow\n",
    "!pip3 install pytesseract\n",
    "!pip3 install transformers\n",
    "!pip3 install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3aa00ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     --------------------------------------- 86.0/86.0 kB 93.2 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (4.29.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.9.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ------------------------------------ 977.6/977.6 kB 107.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2022.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\la\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.2.1)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=ba7b4bf7e2c30f2fd4f75d77e08519d899d116a1f76e118dcc6e6109665890fc\n",
      "  Stored in directory: c:\\users\\la\\appdata\\local\\pip\\cache\\wheels\\71\\67\\06\\162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f4a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as f\n",
    "from PIL import Image,ImageFile\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f5e1b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class textencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "                    nn.Linear(384,256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256,512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128,64)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self,text):\n",
    "        \n",
    "        return self.model(text)\n",
    "        \n",
    "        \n",
    "        \n",
    "### Image Encoder Model (For Getting vector representation of Images)\n",
    "class imageencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(3*100*100,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,64)\n",
    "        )\n",
    "        \n",
    "    def forward(self,image):\n",
    "        return self.model(image)\n",
    "    \n",
    "class MultiModel(nn.Module):\n",
    "    def __init__(self,m1,m2):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32,1),\n",
    "                    nn.Sigmoid(),\n",
    "                    \n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self,image,text):\n",
    "        \n",
    "        image = self.m1(image)\n",
    "        text = self.m2(text)\n",
    "        \n",
    "        combined = torch.cat((image,text),dim=-1)\n",
    "        combined = self.model(combined)\n",
    "        \n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6935900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>imgid</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "      <th>successful</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'plate', 'of', 'delicious', 'food', 'inc...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>a plate of delicious food including French fries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['french', 'fries', 'are', 'not', 'a', 'health...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>French fries are not a healthy food but it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['the', 'plate', 'has', 'one', 'of', 'my', 'fa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>The plate has one of my favorite foods on it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['it', 'was', 'disgusting', 'food', 'not', 'ju...</td>\n",
       "      <td>[0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>It was disgusting food, not just bad food.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31369</td>\n",
       "      <td>train</td>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'plate', 'of', 'disgusting', 'food', 'fo...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>A plate of disgusting food found at a diner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39194</th>\n",
       "      <td>39194</td>\n",
       "      <td>24628</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'dirty', 'bathroom', 'that', 'has', 'a',...</td>\n",
       "      <td>[0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>A dirty bathroom that has a dirty window made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39195</th>\n",
       "      <td>39195</td>\n",
       "      <td>24628</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'dirty', 'bathroom', 'that', 'has', 'a',...</td>\n",
       "      <td>[0.0, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>A dirty bathroom that has a window in it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39196</th>\n",
       "      <td>39196</td>\n",
       "      <td>24628</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'towel', 'that', 'is', 'on', 'a', 'rack'...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>a towel that is on a rack in a dirty bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39197</th>\n",
       "      <td>39197</td>\n",
       "      <td>24628</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'dirty', 'bathroom', 'that', 'has', 'a',...</td>\n",
       "      <td>[0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>A dirty bathroom that has a dirty window made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39198</th>\n",
       "      <td>39198</td>\n",
       "      <td>24628</td>\n",
       "      <td>test</td>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'dirty', 'bathroom', 'that', 'has', 'a',...</td>\n",
       "      <td>[0.0, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>A dirty bathroom that has a window in it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39199 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  imgid  split                       filename  successful  \\\n",
       "0               0  31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "1               1  31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "2               2  31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "3               3  31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "4               4  31369  train  COCO_val2014_000000389081.jpg           1   \n",
       "...           ...    ...    ...                            ...         ...   \n",
       "39194       39194  24628   test  COCO_val2014_000000190705.jpg           1   \n",
       "39195       39195  24628   test  COCO_val2014_000000190705.jpg           1   \n",
       "39196       39196  24628   test  COCO_val2014_000000190705.jpg           1   \n",
       "39197       39197  24628   test  COCO_val2014_000000190705.jpg           1   \n",
       "39198       39198  24628   test  COCO_val2014_000000190705.jpg           1   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['a', 'plate', 'of', 'delicious', 'food', 'inc...   \n",
       "1      ['french', 'fries', 'are', 'not', 'a', 'health...   \n",
       "2      ['the', 'plate', 'has', 'one', 'of', 'my', 'fa...   \n",
       "3      ['it', 'was', 'disgusting', 'food', 'not', 'ju...   \n",
       "4      ['a', 'plate', 'of', 'disgusting', 'food', 'fo...   \n",
       "...                                                  ...   \n",
       "39194  ['a', 'dirty', 'bathroom', 'that', 'has', 'a',...   \n",
       "39195  ['a', 'dirty', 'bathroom', 'that', 'has', 'a',...   \n",
       "39196  ['a', 'towel', 'that', 'is', 'on', 'a', 'rack'...   \n",
       "39197  ['a', 'dirty', 'bathroom', 'that', 'has', 'a',...   \n",
       "39198  ['a', 'dirty', 'bathroom', 'that', 'has', 'a',...   \n",
       "\n",
       "                                          word_sentiment  sentiment  \\\n",
       "0                   [0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0]          1   \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0,...          1   \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0,...          1   \n",
       "3                       [0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]          0   \n",
       "4              [0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0]          0   \n",
       "...                                                  ...        ...   \n",
       "39194   [0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]          0   \n",
       "39195            [0.0, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0]          0   \n",
       "39196  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          0   \n",
       "39197   [0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1]          0   \n",
       "39198            [0.0, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0]          0   \n",
       "\n",
       "                                                     raw  \n",
       "0      a plate of delicious food including French fries.  \n",
       "1      French fries are not a healthy food but it is ...  \n",
       "2      The plate has one of my favorite foods on it, ...  \n",
       "3             It was disgusting food, not just bad food.  \n",
       "4           A plate of disgusting food found at a diner.  \n",
       "...                                                  ...  \n",
       "39194  A dirty bathroom that has a dirty window made ...  \n",
       "39195          A dirty bathroom that has a window in it.  \n",
       "39196      a towel that is on a rack in a dirty bathroom  \n",
       "39197  A dirty bathroom that has a dirty window made ...  \n",
       "39198          A dirty bathroom that has a window in it.  \n",
       "\n",
       "[39199 rows x 9 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "df=pd.read_csv('C:\\\\Users\\\\La\\\\Desktop\\\\sentiment\\\\sentiment.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d62b0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df[['filename', 'raw','sentiment']]\n",
    "idx = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "85f1d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\La\\AppData\\Local\\Temp\\ipykernel_13268\\3933953622.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cols['cleaned_sentences'] = cols['raw'].apply(preprocess_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>a plate of delicious food including French fries.</td>\n",
       "      <td>1</td>\n",
       "      <td>plate delicious food including french fries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>French fries are not a healthy food but it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>french fries healthy food excellent food teena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>The plate has one of my favorite foods on it, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>plate one favorite foods it, french fries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>It was disgusting food, not just bad food.</td>\n",
       "      <td>0</td>\n",
       "      <td>disgusting food, bad food.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>A plate of disgusting food found at a diner.</td>\n",
       "      <td>0</td>\n",
       "      <td>plate disgusting food found diner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39104</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>A dirty bathroom that has a dirty window made ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty bathroom dirty window made dead wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39105</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>A dirty bathroom that has a window in it.</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty bathroom window it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39106</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a towel that is on a rack in a dirty bathroom</td>\n",
       "      <td>0</td>\n",
       "      <td>towel rack dirty bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39107</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>A dirty bathroom that has a dirty window made ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty bathroom dirty window made dead wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39108</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>A dirty bathroom that has a window in it.</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty bathroom window it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39109 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  \\\n",
       "0      COCO_val2014_000000389081.jpg   \n",
       "1      COCO_val2014_000000389081.jpg   \n",
       "2      COCO_val2014_000000389081.jpg   \n",
       "3      COCO_val2014_000000389081.jpg   \n",
       "4      COCO_val2014_000000389081.jpg   \n",
       "...                              ...   \n",
       "39104  COCO_val2014_000000190705.jpg   \n",
       "39105  COCO_val2014_000000190705.jpg   \n",
       "39106  COCO_val2014_000000190705.jpg   \n",
       "39107  COCO_val2014_000000190705.jpg   \n",
       "39108  COCO_val2014_000000190705.jpg   \n",
       "\n",
       "                                                     raw  sentiment  \\\n",
       "0      a plate of delicious food including French fries.          1   \n",
       "1      French fries are not a healthy food but it is ...          1   \n",
       "2      The plate has one of my favorite foods on it, ...          1   \n",
       "3             It was disgusting food, not just bad food.          0   \n",
       "4           A plate of disgusting food found at a diner.          0   \n",
       "...                                                  ...        ...   \n",
       "39104  A dirty bathroom that has a dirty window made ...          0   \n",
       "39105          A dirty bathroom that has a window in it.          0   \n",
       "39106      a towel that is on a rack in a dirty bathroom          0   \n",
       "39107  A dirty bathroom that has a dirty window made ...          0   \n",
       "39108          A dirty bathroom that has a window in it.          0   \n",
       "\n",
       "                                       cleaned_sentences  \n",
       "0           plate delicious food including french fries.  \n",
       "1      french fries healthy food excellent food teena...  \n",
       "2             plate one favorite foods it, french fries.  \n",
       "3                             disgusting food, bad food.  \n",
       "4                     plate disgusting food found diner.  \n",
       "...                                                  ...  \n",
       "39104         dirty bathroom dirty window made dead wood  \n",
       "39105                          dirty bathroom window it.  \n",
       "39106                          towel rack dirty bathroom  \n",
       "39107         dirty bathroom dirty window made dead wood  \n",
       "39108                          dirty bathroom window it.  \n",
       "\n",
       "[39109 rows x 4 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase all sentences, tokenize, remove stopwords and punctuation\n",
    "def preprocess_text(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    sentence = ' '.join(word for word in sentence.split() if word not in stop_words and word not in string.punctuation)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "cols['cleaned_sentences'] = cols['raw'].apply(preprocess_text)\n",
    "\n",
    "idx = df[(df['filename'] == \"COCO_val2014_000000421673.jpg\") | (df['filename'] == \"COCO_val2014_000000130712.jpg\") |(df['filename'] ==\"COCO_val2014_000000310622.jpg\")|(df['filename'] == \"COCO_val2014_000000359276.jpg\")].index\n",
    "cols = cols.drop(idx)\n",
    "cols = cols.reset_index(drop=True)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "35c2a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize([100,100]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7867d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, sentence_model,transform=None, freq_threshold = 5):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.sentence_model = sentence_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open( 'C:\\\\Users\\\\La\\\\Desktop\\\\sentiment\\\\sentiment_images\\\\' + self.df.iloc[idx,0]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        \n",
    "        text = self.df.iloc[idx,3]\n",
    "        text = self.sentence_model.encode(text)\n",
    "        text = torch.tensor(text)\n",
    "        \n",
    "       \n",
    "        label = int(self.df.iloc[idx,2])\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4ffa4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1=imageencoder()\n",
    "M2=textencoder()\n",
    "model = MultiModel(M1, M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a148f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset(cols,sentence_model,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a675e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = torch.utils.data.random_split(data, [31287,7822])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4b10cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x, y, z) in enumerate(dataloader):\n",
    "        output = model(x,y)\n",
    "        loss = loss_fn(output.squeeze(), z.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, z in dataloader:\n",
    "            output = model(x,y)\n",
    "            correct += (output.round() == z).type(torch.float).item()\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, For overall_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "81acc6d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.000837  [    0/31287]\n",
      "loss: 0.001107  [   64/31287]\n",
      "loss: 0.008245  [  128/31287]\n",
      "loss: 0.000008  [  192/31287]\n",
      "loss: 0.000013  [  256/31287]\n",
      "loss: 0.003217  [  320/31287]\n",
      "loss: 0.000103  [  384/31287]\n",
      "loss: 0.000014  [  448/31287]\n",
      "loss: 0.218075  [  512/31287]\n",
      "loss: 0.254646  [  576/31287]\n",
      "loss: 0.000932  [  640/31287]\n",
      "loss: 0.000374  [  704/31287]\n",
      "loss: 0.264846  [  768/31287]\n",
      "loss: 0.000022  [  832/31287]\n",
      "loss: 0.000320  [  896/31287]\n",
      "loss: 0.003254  [  960/31287]\n",
      "loss: 0.502993  [ 1024/31287]\n",
      "loss: 0.001572  [ 1088/31287]\n",
      "loss: 0.016234  [ 1152/31287]\n",
      "loss: 0.001092  [ 1216/31287]\n",
      "loss: 0.001294  [ 1280/31287]\n",
      "loss: 0.001811  [ 1344/31287]\n",
      "loss: 0.014999  [ 1408/31287]\n",
      "loss: 0.013016  [ 1472/31287]\n",
      "loss: 0.054290  [ 1536/31287]\n",
      "loss: 0.003354  [ 1600/31287]\n",
      "loss: 0.006225  [ 1664/31287]\n",
      "loss: 0.012904  [ 1728/31287]\n",
      "loss: 0.041417  [ 1792/31287]\n",
      "loss: 0.000538  [ 1856/31287]\n",
      "loss: 0.000341  [ 1920/31287]\n",
      "loss: 0.002572  [ 1984/31287]\n",
      "loss: 0.000549  [ 2048/31287]\n",
      "loss: 0.000527  [ 2112/31287]\n",
      "loss: 0.006261  [ 2176/31287]\n",
      "loss: 0.005032  [ 2240/31287]\n",
      "loss: 0.000957  [ 2304/31287]\n",
      "loss: 0.000573  [ 2368/31287]\n",
      "loss: 0.000644  [ 2432/31287]\n",
      "loss: 0.000400  [ 2496/31287]\n",
      "loss: 0.002992  [ 2560/31287]\n",
      "loss: 0.002837  [ 2624/31287]\n",
      "loss: 0.000623  [ 2688/31287]\n",
      "loss: 0.007289  [ 2752/31287]\n",
      "loss: 0.066608  [ 2816/31287]\n",
      "loss: 0.003912  [ 2880/31287]\n",
      "loss: 0.001747  [ 2944/31287]\n",
      "loss: 0.002779  [ 3008/31287]\n",
      "loss: 0.002695  [ 3072/31287]\n",
      "loss: 0.001434  [ 3136/31287]\n",
      "loss: 0.024135  [ 3200/31287]\n",
      "loss: 0.000339  [ 3264/31287]\n",
      "loss: 0.000183  [ 3328/31287]\n",
      "loss: 0.002075  [ 3392/31287]\n",
      "loss: 0.000409  [ 3456/31287]\n",
      "loss: 0.000068  [ 3520/31287]\n",
      "loss: 0.000097  [ 3584/31287]\n",
      "loss: 0.003931  [ 3648/31287]\n",
      "loss: 0.056888  [ 3712/31287]\n",
      "loss: 0.028999  [ 3776/31287]\n",
      "loss: 0.001264  [ 3840/31287]\n",
      "loss: 0.001860  [ 3904/31287]\n",
      "loss: 0.002015  [ 3968/31287]\n",
      "loss: 0.026641  [ 4032/31287]\n",
      "loss: 0.000581  [ 4096/31287]\n",
      "loss: 0.000212  [ 4160/31287]\n",
      "loss: 0.127514  [ 4224/31287]\n",
      "loss: 0.088640  [ 4288/31287]\n",
      "loss: 0.000254  [ 4352/31287]\n",
      "loss: 0.002865  [ 4416/31287]\n",
      "loss: 0.000520  [ 4480/31287]\n",
      "loss: 0.000148  [ 4544/31287]\n",
      "loss: 0.000336  [ 4608/31287]\n",
      "loss: 0.008596  [ 4672/31287]\n",
      "loss: 0.000894  [ 4736/31287]\n",
      "loss: 0.000873  [ 4800/31287]\n",
      "loss: 0.004560  [ 4864/31287]\n",
      "loss: 0.000253  [ 4928/31287]\n",
      "loss: 0.004130  [ 4992/31287]\n",
      "loss: 0.026315  [ 5056/31287]\n",
      "loss: 0.026350  [ 5120/31287]\n",
      "loss: 0.002179  [ 5184/31287]\n",
      "loss: 0.002004  [ 5248/31287]\n",
      "loss: 0.004025  [ 5312/31287]\n",
      "loss: 0.215398  [ 5376/31287]\n",
      "loss: 0.006476  [ 5440/31287]\n",
      "loss: 0.000394  [ 5504/31287]\n",
      "loss: 0.007184  [ 5568/31287]\n",
      "loss: 0.001281  [ 5632/31287]\n",
      "loss: 0.000516  [ 5696/31287]\n",
      "loss: 0.000679  [ 5760/31287]\n",
      "loss: 0.003196  [ 5824/31287]\n",
      "loss: 0.034209  [ 5888/31287]\n",
      "loss: 0.044075  [ 5952/31287]\n",
      "loss: 0.000213  [ 6016/31287]\n",
      "loss: 0.000735  [ 6080/31287]\n",
      "loss: 0.002690  [ 6144/31287]\n",
      "loss: 0.000660  [ 6208/31287]\n",
      "loss: 0.000450  [ 6272/31287]\n",
      "loss: 0.232007  [ 6336/31287]\n",
      "loss: 0.000352  [ 6400/31287]\n",
      "loss: 0.003968  [ 6464/31287]\n",
      "loss: 0.000923  [ 6528/31287]\n",
      "loss: 0.001380  [ 6592/31287]\n",
      "loss: 0.069626  [ 6656/31287]\n",
      "loss: 0.003955  [ 6720/31287]\n",
      "loss: 0.017369  [ 6784/31287]\n",
      "loss: 0.004032  [ 6848/31287]\n",
      "loss: 0.007584  [ 6912/31287]\n",
      "loss: 0.003094  [ 6976/31287]\n",
      "loss: 0.015638  [ 7040/31287]\n",
      "loss: 0.001421  [ 7104/31287]\n",
      "loss: 0.001486  [ 7168/31287]\n",
      "loss: 0.037697  [ 7232/31287]\n",
      "loss: 0.000209  [ 7296/31287]\n",
      "loss: 0.000712  [ 7360/31287]\n",
      "loss: 0.022472  [ 7424/31287]\n",
      "loss: 0.007188  [ 7488/31287]\n",
      "loss: 0.002227  [ 7552/31287]\n",
      "loss: 0.002461  [ 7616/31287]\n",
      "loss: 0.184171  [ 7680/31287]\n",
      "loss: 0.016131  [ 7744/31287]\n",
      "loss: 0.033114  [ 7808/31287]\n",
      "loss: 0.066804  [ 7872/31287]\n",
      "loss: 0.004708  [ 7936/31287]\n",
      "loss: 0.100935  [ 8000/31287]\n",
      "loss: 0.012491  [ 8064/31287]\n",
      "loss: 0.001692  [ 8128/31287]\n",
      "loss: 0.008063  [ 8192/31287]\n",
      "loss: 0.001989  [ 8256/31287]\n",
      "loss: 0.003228  [ 8320/31287]\n",
      "loss: 0.002990  [ 8384/31287]\n",
      "loss: 0.001545  [ 8448/31287]\n",
      "loss: 0.000219  [ 8512/31287]\n",
      "loss: 0.001588  [ 8576/31287]\n",
      "loss: 0.010226  [ 8640/31287]\n",
      "loss: 0.003908  [ 8704/31287]\n",
      "loss: 0.000730  [ 8768/31287]\n",
      "loss: 0.005510  [ 8832/31287]\n",
      "loss: 0.000861  [ 8896/31287]\n",
      "loss: 0.003884  [ 8960/31287]\n",
      "loss: 0.000962  [ 9024/31287]\n",
      "loss: 0.000394  [ 9088/31287]\n",
      "loss: 0.000272  [ 9152/31287]\n",
      "loss: 0.000029  [ 9216/31287]\n",
      "loss: 0.023358  [ 9280/31287]\n",
      "loss: 0.000292  [ 9344/31287]\n",
      "loss: 0.000162  [ 9408/31287]\n",
      "loss: 0.001929  [ 9472/31287]\n",
      "loss: 0.000096  [ 9536/31287]\n",
      "loss: 0.000141  [ 9600/31287]\n",
      "loss: 0.000505  [ 9664/31287]\n",
      "loss: 0.017016  [ 9728/31287]\n",
      "loss: 0.000349  [ 9792/31287]\n",
      "loss: 0.000191  [ 9856/31287]\n",
      "loss: 0.083737  [ 9920/31287]\n",
      "loss: 0.000819  [ 9984/31287]\n",
      "loss: 0.003747  [10048/31287]\n",
      "loss: 0.001486  [10112/31287]\n",
      "loss: 0.027071  [10176/31287]\n",
      "loss: 0.011583  [10240/31287]\n",
      "loss: 0.003537  [10304/31287]\n",
      "loss: 0.006832  [10368/31287]\n",
      "loss: 0.004911  [10432/31287]\n",
      "loss: 0.005104  [10496/31287]\n",
      "loss: 0.002815  [10560/31287]\n",
      "loss: 0.001671  [10624/31287]\n",
      "loss: 0.002170  [10688/31287]\n",
      "loss: 0.012111  [10752/31287]\n",
      "loss: 0.055105  [10816/31287]\n",
      "loss: 0.000476  [10880/31287]\n",
      "loss: 0.005262  [10944/31287]\n",
      "loss: 0.000048  [11008/31287]\n",
      "loss: 0.035898  [11072/31287]\n",
      "loss: 0.001730  [11136/31287]\n",
      "loss: 0.001699  [11200/31287]\n",
      "loss: 0.005384  [11264/31287]\n",
      "loss: 0.005283  [11328/31287]\n",
      "loss: 0.051316  [11392/31287]\n",
      "loss: 0.001388  [11456/31287]\n",
      "loss: 0.004101  [11520/31287]\n",
      "loss: 0.001015  [11584/31287]\n",
      "loss: 0.001923  [11648/31287]\n",
      "loss: 0.000482  [11712/31287]\n",
      "loss: 0.000234  [11776/31287]\n",
      "loss: 0.000482  [11840/31287]\n",
      "loss: 0.001068  [11904/31287]\n",
      "loss: 0.000095  [11968/31287]\n",
      "loss: 0.034389  [12032/31287]\n",
      "loss: 0.000077  [12096/31287]\n",
      "loss: 0.004057  [12160/31287]\n",
      "loss: 0.000247  [12224/31287]\n",
      "loss: 0.032902  [12288/31287]\n",
      "loss: 0.035771  [12352/31287]\n",
      "loss: 0.000137  [12416/31287]\n",
      "loss: 0.010015  [12480/31287]\n",
      "loss: 0.001487  [12544/31287]\n",
      "loss: 0.000463  [12608/31287]\n",
      "loss: 0.003614  [12672/31287]\n",
      "loss: 0.006533  [12736/31287]\n",
      "loss: 0.000594  [12800/31287]\n",
      "loss: 0.001903  [12864/31287]\n",
      "loss: 0.027345  [12928/31287]\n",
      "loss: 0.215925  [12992/31287]\n",
      "loss: 0.000749  [13056/31287]\n",
      "loss: 0.011476  [13120/31287]\n",
      "loss: 0.017535  [13184/31287]\n",
      "loss: 0.004051  [13248/31287]\n",
      "loss: 0.013465  [13312/31287]\n",
      "loss: 0.003248  [13376/31287]\n",
      "loss: 0.015378  [13440/31287]\n",
      "loss: 0.014301  [13504/31287]\n",
      "loss: 0.002942  [13568/31287]\n",
      "loss: 0.002974  [13632/31287]\n",
      "loss: 0.001200  [13696/31287]\n",
      "loss: 0.275382  [13760/31287]\n",
      "loss: 0.015587  [13824/31287]\n",
      "loss: 0.003631  [13888/31287]\n",
      "loss: 0.000614  [13952/31287]\n",
      "loss: 0.000623  [14016/31287]\n",
      "loss: 0.007384  [14080/31287]\n",
      "loss: 0.001512  [14144/31287]\n",
      "loss: 0.038139  [14208/31287]\n",
      "loss: 0.004558  [14272/31287]\n",
      "loss: 0.004493  [14336/31287]\n",
      "loss: 0.047303  [14400/31287]\n",
      "loss: 0.000507  [14464/31287]\n",
      "loss: 0.001272  [14528/31287]\n",
      "loss: 0.210895  [14592/31287]\n",
      "loss: 0.012720  [14656/31287]\n",
      "loss: 0.001764  [14720/31287]\n",
      "loss: 0.003394  [14784/31287]\n",
      "loss: 0.103970  [14848/31287]\n",
      "loss: 0.022061  [14912/31287]\n",
      "loss: 0.166095  [14976/31287]\n",
      "loss: 0.005366  [15040/31287]\n",
      "loss: 0.005519  [15104/31287]\n",
      "loss: 0.005601  [15168/31287]\n",
      "loss: 0.046726  [15232/31287]\n",
      "loss: 0.001980  [15296/31287]\n",
      "loss: 0.004676  [15360/31287]\n",
      "loss: 0.000823  [15424/31287]\n",
      "loss: 0.003895  [15488/31287]\n",
      "loss: 0.001205  [15552/31287]\n",
      "loss: 0.001552  [15616/31287]\n",
      "loss: 0.008482  [15680/31287]\n",
      "loss: 0.000417  [15744/31287]\n",
      "loss: 0.000064  [15808/31287]\n",
      "loss: 0.001486  [15872/31287]\n",
      "loss: 0.160675  [15936/31287]\n",
      "loss: 0.000003  [16000/31287]\n",
      "loss: 0.000108  [16064/31287]\n",
      "loss: 0.000329  [16128/31287]\n",
      "loss: 0.005498  [16192/31287]\n",
      "loss: 0.000011  [16256/31287]\n",
      "loss: 0.000546  [16320/31287]\n",
      "loss: 0.000128  [16384/31287]\n",
      "loss: 0.259256  [16448/31287]\n",
      "loss: 0.138919  [16512/31287]\n",
      "loss: 0.000399  [16576/31287]\n",
      "loss: 0.000449  [16640/31287]\n",
      "loss: 0.006273  [16704/31287]\n",
      "loss: 0.034361  [16768/31287]\n",
      "loss: 0.004188  [16832/31287]\n",
      "loss: 0.004846  [16896/31287]\n",
      "loss: 0.009161  [16960/31287]\n",
      "loss: 0.002547  [17024/31287]\n",
      "loss: 0.011532  [17088/31287]\n",
      "loss: 0.001519  [17152/31287]\n",
      "loss: 0.001401  [17216/31287]\n",
      "loss: 0.063785  [17280/31287]\n",
      "loss: 0.001328  [17344/31287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000778  [17408/31287]\n",
      "loss: 0.005218  [17472/31287]\n",
      "loss: 0.006346  [17536/31287]\n",
      "loss: 0.000347  [17600/31287]\n",
      "loss: 0.000499  [17664/31287]\n",
      "loss: 0.049868  [17728/31287]\n",
      "loss: 0.000193  [17792/31287]\n",
      "loss: 0.005317  [17856/31287]\n",
      "loss: 0.000313  [17920/31287]\n",
      "loss: 0.001113  [17984/31287]\n",
      "loss: 0.023407  [18048/31287]\n",
      "loss: 0.263257  [18112/31287]\n",
      "loss: 0.003935  [18176/31287]\n",
      "loss: 0.003178  [18240/31287]\n",
      "loss: 0.004411  [18304/31287]\n",
      "loss: 0.008012  [18368/31287]\n",
      "loss: 0.200841  [18432/31287]\n",
      "loss: 0.000458  [18496/31287]\n",
      "loss: 0.006266  [18560/31287]\n",
      "loss: 0.002575  [18624/31287]\n",
      "loss: 0.004063  [18688/31287]\n",
      "loss: 0.008309  [18752/31287]\n",
      "loss: 0.039894  [18816/31287]\n",
      "loss: 0.006939  [18880/31287]\n",
      "loss: 0.003242  [18944/31287]\n",
      "loss: 0.002681  [19008/31287]\n",
      "loss: 0.003037  [19072/31287]\n",
      "loss: 0.009401  [19136/31287]\n",
      "loss: 0.006436  [19200/31287]\n",
      "loss: 0.011838  [19264/31287]\n",
      "loss: 0.008015  [19328/31287]\n",
      "loss: 0.000922  [19392/31287]\n",
      "loss: 0.000895  [19456/31287]\n",
      "loss: 0.001550  [19520/31287]\n",
      "loss: 0.002107  [19584/31287]\n",
      "loss: 0.000653  [19648/31287]\n",
      "loss: 0.000835  [19712/31287]\n",
      "loss: 0.001012  [19776/31287]\n",
      "loss: 0.231441  [19840/31287]\n",
      "loss: 0.000835  [19904/31287]\n",
      "loss: 0.001154  [19968/31287]\n",
      "loss: 0.001931  [20032/31287]\n",
      "loss: 0.085720  [20096/31287]\n",
      "loss: 0.005825  [20160/31287]\n",
      "loss: 0.001720  [20224/31287]\n",
      "loss: 0.000472  [20288/31287]\n",
      "loss: 0.001039  [20352/31287]\n",
      "loss: 0.014173  [20416/31287]\n",
      "loss: 0.004787  [20480/31287]\n",
      "loss: 0.012296  [20544/31287]\n",
      "loss: 0.005772  [20608/31287]\n",
      "loss: 0.000859  [20672/31287]\n",
      "loss: 0.001703  [20736/31287]\n",
      "loss: 0.000279  [20800/31287]\n",
      "loss: 0.000502  [20864/31287]\n",
      "loss: 0.000183  [20928/31287]\n",
      "loss: 0.000966  [20992/31287]\n",
      "loss: 0.004917  [21056/31287]\n",
      "loss: 0.000292  [21120/31287]\n",
      "loss: 0.000619  [21184/31287]\n",
      "loss: 0.003385  [21248/31287]\n",
      "loss: 0.000511  [21312/31287]\n",
      "loss: 0.000164  [21376/31287]\n",
      "loss: 0.008071  [21440/31287]\n",
      "loss: 0.000340  [21504/31287]\n",
      "loss: 0.000447  [21568/31287]\n",
      "loss: 0.000124  [21632/31287]\n",
      "loss: 0.008206  [21696/31287]\n",
      "loss: 0.000612  [21760/31287]\n",
      "loss: 0.000284  [21824/31287]\n",
      "loss: 0.003291  [21888/31287]\n",
      "loss: 0.014489  [21952/31287]\n",
      "loss: 0.000300  [22016/31287]\n",
      "loss: 0.000171  [22080/31287]\n",
      "loss: 0.000065  [22144/31287]\n",
      "loss: 0.009813  [22208/31287]\n",
      "loss: 0.000408  [22272/31287]\n",
      "loss: 0.002629  [22336/31287]\n",
      "loss: 0.000449  [22400/31287]\n",
      "loss: 0.000065  [22464/31287]\n",
      "loss: 0.000718  [22528/31287]\n",
      "loss: 0.002298  [22592/31287]\n",
      "loss: 0.000041  [22656/31287]\n",
      "loss: 0.000484  [22720/31287]\n",
      "loss: 0.000005  [22784/31287]\n",
      "loss: 0.000004  [22848/31287]\n",
      "loss: 0.000007  [22912/31287]\n",
      "loss: 0.000014  [22976/31287]\n",
      "loss: 0.126481  [23040/31287]\n",
      "loss: 0.000058  [23104/31287]\n",
      "loss: 0.000162  [23168/31287]\n",
      "loss: 0.003343  [23232/31287]\n",
      "loss: 0.015890  [23296/31287]\n",
      "loss: 0.000482  [23360/31287]\n",
      "loss: 0.000326  [23424/31287]\n",
      "loss: 0.013148  [23488/31287]\n",
      "loss: 0.000151  [23552/31287]\n",
      "loss: 0.005069  [23616/31287]\n",
      "loss: 0.000602  [23680/31287]\n",
      "loss: 0.001304  [23744/31287]\n",
      "loss: 0.000510  [23808/31287]\n",
      "loss: 0.000111  [23872/31287]\n",
      "loss: 0.000479  [23936/31287]\n",
      "loss: 0.000122  [24000/31287]\n",
      "loss: 0.059231  [24064/31287]\n",
      "loss: 0.000906  [24128/31287]\n",
      "loss: 0.000656  [24192/31287]\n",
      "loss: 0.000100  [24256/31287]\n",
      "loss: 0.000258  [24320/31287]\n",
      "loss: 0.001248  [24384/31287]\n",
      "loss: 0.009041  [24448/31287]\n",
      "loss: 0.000382  [24512/31287]\n",
      "loss: 0.000325  [24576/31287]\n",
      "loss: 0.000443  [24640/31287]\n",
      "loss: 0.004998  [24704/31287]\n",
      "loss: 0.003806  [24768/31287]\n",
      "loss: 0.001993  [24832/31287]\n",
      "loss: 0.006593  [24896/31287]\n",
      "loss: 0.009351  [24960/31287]\n",
      "loss: 0.000913  [25024/31287]\n",
      "loss: 0.000885  [25088/31287]\n",
      "loss: 0.000441  [25152/31287]\n",
      "loss: 0.022071  [25216/31287]\n",
      "loss: 0.005247  [25280/31287]\n",
      "loss: 0.002161  [25344/31287]\n",
      "loss: 0.000701  [25408/31287]\n",
      "loss: 0.000948  [25472/31287]\n",
      "loss: 0.000266  [25536/31287]\n",
      "loss: 0.000657  [25600/31287]\n",
      "loss: 0.000916  [25664/31287]\n",
      "loss: 0.001172  [25728/31287]\n",
      "loss: 0.001555  [25792/31287]\n",
      "loss: 0.000435  [25856/31287]\n",
      "loss: 0.000004  [25920/31287]\n",
      "loss: 0.000666  [25984/31287]\n",
      "loss: 0.000010  [26048/31287]\n",
      "loss: 0.000151  [26112/31287]\n",
      "loss: 0.001555  [26176/31287]\n",
      "loss: 0.000251  [26240/31287]\n",
      "loss: 0.000179  [26304/31287]\n",
      "loss: 0.002564  [26368/31287]\n",
      "loss: 0.000686  [26432/31287]\n",
      "loss: 0.000048  [26496/31287]\n",
      "loss: 0.000028  [26560/31287]\n",
      "loss: 0.000101  [26624/31287]\n",
      "loss: 0.009174  [26688/31287]\n",
      "loss: 0.000566  [26752/31287]\n",
      "loss: 0.000122  [26816/31287]\n",
      "loss: 0.000142  [26880/31287]\n",
      "loss: 0.000041  [26944/31287]\n",
      "loss: 0.000520  [27008/31287]\n",
      "loss: 0.000012  [27072/31287]\n",
      "loss: 0.000252  [27136/31287]\n",
      "loss: 0.000063  [27200/31287]\n",
      "loss: 0.000038  [27264/31287]\n",
      "loss: 0.000262  [27328/31287]\n",
      "loss: 0.000008  [27392/31287]\n",
      "loss: 0.003080  [27456/31287]\n",
      "loss: 0.038676  [27520/31287]\n",
      "loss: 0.000150  [27584/31287]\n",
      "loss: 0.000624  [27648/31287]\n",
      "loss: 0.313206  [27712/31287]\n",
      "loss: 0.000005  [27776/31287]\n",
      "loss: 0.000234  [27840/31287]\n",
      "loss: 0.000055  [27904/31287]\n",
      "loss: 0.000373  [27968/31287]\n",
      "loss: 0.000126  [28032/31287]\n",
      "loss: 0.026595  [28096/31287]\n",
      "loss: 0.000465  [28160/31287]\n",
      "loss: 0.003257  [28224/31287]\n",
      "loss: 0.000665  [28288/31287]\n",
      "loss: 0.022217  [28352/31287]\n",
      "loss: 0.001228  [28416/31287]\n",
      "loss: 0.008309  [28480/31287]\n",
      "loss: 0.002082  [28544/31287]\n",
      "loss: 0.138636  [28608/31287]\n",
      "loss: 0.000484  [28672/31287]\n",
      "loss: 0.001828  [28736/31287]\n",
      "loss: 0.017974  [28800/31287]\n",
      "loss: 0.000382  [28864/31287]\n",
      "loss: 0.000433  [28928/31287]\n",
      "loss: 0.000556  [28992/31287]\n",
      "loss: 0.003911  [29056/31287]\n",
      "loss: 0.000764  [29120/31287]\n",
      "loss: 0.003788  [29184/31287]\n",
      "loss: 0.000829  [29248/31287]\n",
      "loss: 0.000287  [29312/31287]\n",
      "loss: 0.000347  [29376/31287]\n",
      "loss: 0.001053  [29440/31287]\n",
      "loss: 0.000417  [29504/31287]\n",
      "loss: 0.000129  [29568/31287]\n",
      "loss: 0.000071  [29632/31287]\n",
      "loss: 0.000326  [29696/31287]\n",
      "loss: 0.001038  [29760/31287]\n",
      "loss: 0.000462  [29824/31287]\n",
      "loss: 0.000450  [29888/31287]\n",
      "loss: 0.000111  [29952/31287]\n",
      "loss: 0.000136  [30016/31287]\n",
      "loss: 0.000342  [30080/31287]\n",
      "loss: 0.000314  [30144/31287]\n",
      "loss: 0.001242  [30208/31287]\n",
      "loss: 0.363150  [30272/31287]\n",
      "loss: 0.000225  [30336/31287]\n",
      "loss: 0.049600  [30400/31287]\n",
      "loss: 0.001694  [30464/31287]\n",
      "loss: 0.115338  [30528/31287]\n",
      "loss: 0.002602  [30592/31287]\n",
      "loss: 0.015363  [30656/31287]\n",
      "loss: 0.012906  [30720/31287]\n",
      "loss: 0.006895  [30784/31287]\n",
      "loss: 0.000661  [30848/31287]\n",
      "loss: 0.003631  [30912/31287]\n",
      "loss: 0.009745  [30976/31287]\n",
      "loss: 0.024527  [31040/31287]\n",
      "loss: 0.004730  [31104/31287]\n",
      "loss: 0.004889  [31168/31287]\n",
      "loss: 0.048050  [31232/31287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.87%\n",
      "Precision: 0.22\n",
      "Recall: 0.47\n",
      "F1 Score: 0.30\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000800  [    0/31287]\n",
      "loss: 0.022123  [   64/31287]\n",
      "loss: 0.007830  [  128/31287]\n",
      "loss: 0.000191  [  192/31287]\n",
      "loss: 0.006922  [  256/31287]\n",
      "loss: 0.001874  [  320/31287]\n",
      "loss: 0.003702  [  384/31287]\n",
      "loss: 0.001285  [  448/31287]\n",
      "loss: 0.013292  [  512/31287]\n",
      "loss: 0.054066  [  576/31287]\n",
      "loss: 0.023851  [  640/31287]\n",
      "loss: 0.002806  [  704/31287]\n",
      "loss: 0.005931  [  768/31287]\n",
      "loss: 0.000037  [  832/31287]\n",
      "loss: 0.000219  [  896/31287]\n",
      "loss: 0.009769  [  960/31287]\n",
      "loss: 0.000542  [ 1024/31287]\n",
      "loss: 0.004334  [ 1088/31287]\n",
      "loss: 0.000892  [ 1152/31287]\n",
      "loss: 0.051126  [ 1216/31287]\n",
      "loss: 0.006352  [ 1280/31287]\n",
      "loss: 0.000281  [ 1344/31287]\n",
      "loss: 0.000347  [ 1408/31287]\n",
      "loss: 0.000073  [ 1472/31287]\n",
      "loss: 0.000084  [ 1536/31287]\n",
      "loss: 0.000712  [ 1600/31287]\n",
      "loss: 0.000040  [ 1664/31287]\n",
      "loss: 0.001323  [ 1728/31287]\n",
      "loss: 0.001853  [ 1792/31287]\n",
      "loss: 0.000135  [ 1856/31287]\n",
      "loss: 0.001833  [ 1920/31287]\n",
      "loss: 0.000750  [ 1984/31287]\n",
      "loss: 0.000010  [ 2048/31287]\n",
      "loss: 0.006527  [ 2112/31287]\n",
      "loss: 0.034389  [ 2176/31287]\n",
      "loss: 0.010188  [ 2240/31287]\n",
      "loss: 0.007308  [ 2304/31287]\n",
      "loss: 0.000287  [ 2368/31287]\n",
      "loss: 0.001384  [ 2432/31287]\n",
      "loss: 0.000730  [ 2496/31287]\n",
      "loss: 0.051114  [ 2560/31287]\n",
      "loss: 0.039981  [ 2624/31287]\n",
      "loss: 0.002123  [ 2688/31287]\n",
      "loss: 0.001159  [ 2752/31287]\n",
      "loss: 0.018541  [ 2816/31287]\n",
      "loss: 0.027467  [ 2880/31287]\n",
      "loss: 0.002114  [ 2944/31287]\n",
      "loss: 0.126451  [ 3008/31287]\n",
      "loss: 0.001925  [ 3072/31287]\n",
      "loss: 0.012256  [ 3136/31287]\n",
      "loss: 0.014962  [ 3200/31287]\n",
      "loss: 0.007886  [ 3264/31287]\n",
      "loss: 0.007776  [ 3328/31287]\n",
      "loss: 0.002195  [ 3392/31287]\n",
      "loss: 0.003945  [ 3456/31287]\n",
      "loss: 0.005033  [ 3520/31287]\n",
      "loss: 0.000991  [ 3584/31287]\n",
      "loss: 0.001793  [ 3648/31287]\n",
      "loss: 0.003611  [ 3712/31287]\n",
      "loss: 0.028388  [ 3776/31287]\n",
      "loss: 0.000766  [ 3840/31287]\n",
      "loss: 0.096843  [ 3904/31287]\n",
      "loss: 0.001166  [ 3968/31287]\n",
      "loss: 0.001837  [ 4032/31287]\n",
      "loss: 0.000632  [ 4096/31287]\n",
      "loss: 0.000402  [ 4160/31287]\n",
      "loss: 0.001545  [ 4224/31287]\n",
      "loss: 0.000975  [ 4288/31287]\n",
      "loss: 0.000102  [ 4352/31287]\n",
      "loss: 0.000957  [ 4416/31287]\n",
      "loss: 0.000861  [ 4480/31287]\n",
      "loss: 0.000136  [ 4544/31287]\n",
      "loss: 0.000218  [ 4608/31287]\n",
      "loss: 0.000725  [ 4672/31287]\n",
      "loss: 0.000431  [ 4736/31287]\n",
      "loss: 0.000614  [ 4800/31287]\n",
      "loss: 0.001169  [ 4864/31287]\n",
      "loss: 0.000176  [ 4928/31287]\n",
      "loss: 0.000132  [ 4992/31287]\n",
      "loss: 0.000929  [ 5056/31287]\n",
      "loss: 0.001936  [ 5120/31287]\n",
      "loss: 0.000349  [ 5184/31287]\n",
      "loss: 0.000739  [ 5248/31287]\n",
      "loss: 0.000279  [ 5312/31287]\n",
      "loss: 0.202741  [ 5376/31287]\n",
      "loss: 0.002181  [ 5440/31287]\n",
      "loss: 0.000534  [ 5504/31287]\n",
      "loss: 0.001152  [ 5568/31287]\n",
      "loss: 0.004034  [ 5632/31287]\n",
      "loss: 0.000217  [ 5696/31287]\n",
      "loss: 0.000853  [ 5760/31287]\n",
      "loss: 0.000311  [ 5824/31287]\n",
      "loss: 0.003771  [ 5888/31287]\n",
      "loss: 0.022963  [ 5952/31287]\n",
      "loss: 0.003368  [ 6016/31287]\n",
      "loss: 0.001582  [ 6080/31287]\n",
      "loss: 0.009764  [ 6144/31287]\n",
      "loss: 0.057240  [ 6208/31287]\n",
      "loss: 0.002953  [ 6272/31287]\n",
      "loss: 0.115834  [ 6336/31287]\n",
      "loss: 0.000216  [ 6400/31287]\n",
      "loss: 0.004129  [ 6464/31287]\n",
      "loss: 0.002356  [ 6528/31287]\n",
      "loss: 0.001410  [ 6592/31287]\n",
      "loss: 0.005803  [ 6656/31287]\n",
      "loss: 0.006016  [ 6720/31287]\n",
      "loss: 0.003026  [ 6784/31287]\n",
      "loss: 0.001513  [ 6848/31287]\n",
      "loss: 0.000600  [ 6912/31287]\n",
      "loss: 0.001768  [ 6976/31287]\n",
      "loss: 0.000744  [ 7040/31287]\n",
      "loss: 0.000744  [ 7104/31287]\n",
      "loss: 0.011550  [ 7168/31287]\n",
      "loss: 0.002111  [ 7232/31287]\n",
      "loss: 0.000072  [ 7296/31287]\n",
      "loss: 0.000099  [ 7360/31287]\n",
      "loss: 0.000007  [ 7424/31287]\n",
      "loss: 0.000186  [ 7488/31287]\n",
      "loss: 0.000096  [ 7552/31287]\n",
      "loss: 0.000113  [ 7616/31287]\n",
      "loss: 0.002519  [ 7680/31287]\n",
      "loss: 0.037945  [ 7744/31287]\n",
      "loss: 0.000285  [ 7808/31287]\n",
      "loss: 0.003354  [ 7872/31287]\n",
      "loss: 0.000011  [ 7936/31287]\n",
      "loss: 0.000093  [ 8000/31287]\n",
      "loss: 0.000011  [ 8064/31287]\n",
      "loss: 0.001491  [ 8128/31287]\n",
      "loss: 0.003057  [ 8192/31287]\n",
      "loss: 0.000013  [ 8256/31287]\n",
      "loss: 0.184262  [ 8320/31287]\n",
      "loss: 0.002340  [ 8384/31287]\n",
      "loss: 0.000094  [ 8448/31287]\n",
      "loss: 0.000015  [ 8512/31287]\n",
      "loss: 0.000128  [ 8576/31287]\n",
      "loss: 0.106330  [ 8640/31287]\n",
      "loss: 0.000021  [ 8704/31287]\n",
      "loss: 0.000499  [ 8768/31287]\n",
      "loss: 0.000991  [ 8832/31287]\n",
      "loss: 0.001048  [ 8896/31287]\n",
      "loss: 0.001817  [ 8960/31287]\n",
      "loss: 0.055094  [ 9024/31287]\n",
      "loss: 0.099008  [ 9088/31287]\n",
      "loss: 0.002760  [ 9152/31287]\n",
      "loss: 0.000289  [ 9216/31287]\n",
      "loss: 0.002640  [ 9280/31287]\n",
      "loss: 0.097821  [ 9344/31287]\n",
      "loss: 0.005321  [ 9408/31287]\n",
      "loss: 0.003734  [ 9472/31287]\n",
      "loss: 0.004575  [ 9536/31287]\n",
      "loss: 0.002220  [ 9600/31287]\n",
      "loss: 0.004682  [ 9664/31287]\n",
      "loss: 0.001192  [ 9728/31287]\n",
      "loss: 0.001283  [ 9792/31287]\n",
      "loss: 0.000563  [ 9856/31287]\n",
      "loss: 0.003883  [ 9920/31287]\n",
      "loss: 0.001724  [ 9984/31287]\n",
      "loss: 0.003223  [10048/31287]\n",
      "loss: 0.001432  [10112/31287]\n",
      "loss: 0.002128  [10176/31287]\n",
      "loss: 0.006806  [10240/31287]\n",
      "loss: 0.001181  [10304/31287]\n",
      "loss: 0.000930  [10368/31287]\n",
      "loss: 0.000655  [10432/31287]\n",
      "loss: 0.001284  [10496/31287]\n",
      "loss: 0.001463  [10560/31287]\n",
      "loss: 0.000603  [10624/31287]\n",
      "loss: 0.001147  [10688/31287]\n",
      "loss: 0.007544  [10752/31287]\n",
      "loss: 0.019322  [10816/31287]\n",
      "loss: 0.000420  [10880/31287]\n",
      "loss: 0.004940  [10944/31287]\n",
      "loss: 0.000065  [11008/31287]\n",
      "loss: 0.001498  [11072/31287]\n",
      "loss: 0.000731  [11136/31287]\n",
      "loss: 0.000144  [11200/31287]\n",
      "loss: 0.003983  [11264/31287]\n",
      "loss: 0.000514  [11328/31287]\n",
      "loss: 0.154833  [11392/31287]\n",
      "loss: 0.000504  [11456/31287]\n",
      "loss: 0.004204  [11520/31287]\n",
      "loss: 0.000760  [11584/31287]\n",
      "loss: 0.003673  [11648/31287]\n",
      "loss: 0.000523  [11712/31287]\n",
      "loss: 0.000363  [11776/31287]\n",
      "loss: 0.001846  [11840/31287]\n",
      "loss: 0.001216  [11904/31287]\n",
      "loss: 0.001044  [11968/31287]\n",
      "loss: 0.002706  [12032/31287]\n",
      "loss: 0.002231  [12096/31287]\n",
      "loss: 0.076558  [12160/31287]\n",
      "loss: 0.008914  [12224/31287]\n",
      "loss: 0.016490  [12288/31287]\n",
      "loss: 0.003331  [12352/31287]\n",
      "loss: 0.002341  [12416/31287]\n",
      "loss: 0.010759  [12480/31287]\n",
      "loss: 0.002073  [12544/31287]\n",
      "loss: 0.000707  [12608/31287]\n",
      "loss: 0.043363  [12672/31287]\n",
      "loss: 0.006070  [12736/31287]\n",
      "loss: 0.001620  [12800/31287]\n",
      "loss: 0.002409  [12864/31287]\n",
      "loss: 0.056491  [12928/31287]\n",
      "loss: 0.027382  [12992/31287]\n",
      "loss: 0.003696  [13056/31287]\n",
      "loss: 0.000372  [13120/31287]\n",
      "loss: 0.007150  [13184/31287]\n",
      "loss: 0.000263  [13248/31287]\n",
      "loss: 0.000023  [13312/31287]\n",
      "loss: 0.000150  [13376/31287]\n",
      "loss: 0.201123  [13440/31287]\n",
      "loss: 0.002202  [13504/31287]\n",
      "loss: 0.000820  [13568/31287]\n",
      "loss: 0.001450  [13632/31287]\n",
      "loss: 0.000012  [13696/31287]\n",
      "loss: 0.000161  [13760/31287]\n",
      "loss: 0.000815  [13824/31287]\n",
      "loss: 0.000272  [13888/31287]\n",
      "loss: 0.000449  [13952/31287]\n",
      "loss: 0.000026  [14016/31287]\n",
      "loss: 0.002455  [14080/31287]\n",
      "loss: 0.002537  [14144/31287]\n",
      "loss: 0.013724  [14208/31287]\n",
      "loss: 0.006215  [14272/31287]\n",
      "loss: 0.010687  [14336/31287]\n",
      "loss: 0.033019  [14400/31287]\n",
      "loss: 0.000237  [14464/31287]\n",
      "loss: 0.002565  [14528/31287]\n",
      "loss: 0.000640  [14592/31287]\n",
      "loss: 0.002742  [14656/31287]\n",
      "loss: 0.000986  [14720/31287]\n",
      "loss: 0.126612  [14784/31287]\n",
      "loss: 0.002395  [14848/31287]\n",
      "loss: 0.000587  [14912/31287]\n",
      "loss: 0.001009  [14976/31287]\n",
      "loss: 0.004243  [15040/31287]\n",
      "loss: 0.008467  [15104/31287]\n",
      "loss: 0.014816  [15168/31287]\n",
      "loss: 0.004333  [15232/31287]\n",
      "loss: 0.010207  [15296/31287]\n",
      "loss: 0.014588  [15360/31287]\n",
      "loss: 0.001600  [15424/31287]\n",
      "loss: 0.070306  [15488/31287]\n",
      "loss: 0.001527  [15552/31287]\n",
      "loss: 0.002424  [15616/31287]\n",
      "loss: 0.016564  [15680/31287]\n",
      "loss: 0.002069  [15744/31287]\n",
      "loss: 0.003807  [15808/31287]\n",
      "loss: 0.001409  [15872/31287]\n",
      "loss: 0.003869  [15936/31287]\n",
      "loss: 0.000211  [16000/31287]\n",
      "loss: 0.000914  [16064/31287]\n",
      "loss: 0.002928  [16128/31287]\n",
      "loss: 0.096106  [16192/31287]\n",
      "loss: 0.003987  [16256/31287]\n",
      "loss: 0.007148  [16320/31287]\n",
      "loss: 0.002741  [16384/31287]\n",
      "loss: 0.009880  [16448/31287]\n",
      "loss: 0.006010  [16512/31287]\n",
      "loss: 0.001437  [16576/31287]\n",
      "loss: 0.000935  [16640/31287]\n",
      "loss: 0.004497  [16704/31287]\n",
      "loss: 0.001828  [16768/31287]\n",
      "loss: 0.002122  [16832/31287]\n",
      "loss: 0.001831  [16896/31287]\n",
      "loss: 0.020266  [16960/31287]\n",
      "loss: 0.004476  [17024/31287]\n",
      "loss: 0.000250  [17088/31287]\n",
      "loss: 0.001403  [17152/31287]\n",
      "loss: 0.000553  [17216/31287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.001597  [17280/31287]\n",
      "loss: 0.002830  [17344/31287]\n",
      "loss: 0.000151  [17408/31287]\n",
      "loss: 0.000869  [17472/31287]\n",
      "loss: 0.005505  [17536/31287]\n",
      "loss: 0.005563  [17600/31287]\n",
      "loss: 0.000783  [17664/31287]\n",
      "loss: 0.000114  [17728/31287]\n",
      "loss: 0.000058  [17792/31287]\n",
      "loss: 0.067949  [17856/31287]\n",
      "loss: 0.000032  [17920/31287]\n",
      "loss: 0.002024  [17984/31287]\n",
      "loss: 0.073705  [18048/31287]\n",
      "loss: 0.018185  [18112/31287]\n",
      "loss: 0.014208  [18176/31287]\n",
      "loss: 0.000328  [18240/31287]\n",
      "loss: 0.001462  [18304/31287]\n",
      "loss: 0.000629  [18368/31287]\n",
      "loss: 0.002479  [18432/31287]\n",
      "loss: 0.000019  [18496/31287]\n",
      "loss: 0.002738  [18560/31287]\n",
      "loss: 0.000266  [18624/31287]\n",
      "loss: 0.000511  [18688/31287]\n",
      "loss: 0.001404  [18752/31287]\n",
      "loss: 0.001773  [18816/31287]\n",
      "loss: 0.088937  [18880/31287]\n",
      "loss: 0.000654  [18944/31287]\n",
      "loss: 0.000847  [19008/31287]\n",
      "loss: 0.000262  [19072/31287]\n",
      "loss: 0.000326  [19136/31287]\n",
      "loss: 0.001825  [19200/31287]\n",
      "loss: 0.001036  [19264/31287]\n",
      "loss: 0.042221  [19328/31287]\n",
      "loss: 0.001755  [19392/31287]\n",
      "loss: 0.000111  [19456/31287]\n",
      "loss: 0.000090  [19520/31287]\n",
      "loss: 0.000160  [19584/31287]\n",
      "loss: 0.001010  [19648/31287]\n",
      "loss: 0.001736  [19712/31287]\n",
      "loss: 0.007149  [19776/31287]\n",
      "loss: 0.000543  [19840/31287]\n",
      "loss: 0.008053  [19904/31287]\n",
      "loss: 0.000286  [19968/31287]\n",
      "loss: 0.000143  [20032/31287]\n",
      "loss: 0.000751  [20096/31287]\n",
      "loss: 0.001367  [20160/31287]\n",
      "loss: 0.000141  [20224/31287]\n",
      "loss: 0.000204  [20288/31287]\n",
      "loss: 0.000123  [20352/31287]\n",
      "loss: 0.000248  [20416/31287]\n",
      "loss: 0.000079  [20480/31287]\n",
      "loss: 0.000025  [20544/31287]\n",
      "loss: 0.000269  [20608/31287]\n",
      "loss: 0.000214  [20672/31287]\n",
      "loss: 0.000687  [20736/31287]\n",
      "loss: 0.000264  [20800/31287]\n",
      "loss: 0.000108  [20864/31287]\n",
      "loss: 0.000162  [20928/31287]\n",
      "loss: 0.000287  [20992/31287]\n",
      "loss: 0.000167  [21056/31287]\n",
      "loss: 0.000199  [21120/31287]\n",
      "loss: 0.000099  [21184/31287]\n",
      "loss: 0.000896  [21248/31287]\n",
      "loss: 0.000053  [21312/31287]\n",
      "loss: 0.000336  [21376/31287]\n",
      "loss: 0.001496  [21440/31287]\n",
      "loss: 0.003898  [21504/31287]\n",
      "loss: 0.000205  [21568/31287]\n",
      "loss: 0.000216  [21632/31287]\n",
      "loss: 0.000269  [21696/31287]\n",
      "loss: 0.000046  [21760/31287]\n",
      "loss: 0.000045  [21824/31287]\n",
      "loss: 0.000744  [21888/31287]\n",
      "loss: 0.000474  [21952/31287]\n",
      "loss: 0.000974  [22016/31287]\n",
      "loss: 0.000693  [22080/31287]\n",
      "loss: 0.000032  [22144/31287]\n",
      "loss: 0.000628  [22208/31287]\n",
      "loss: 0.001169  [22272/31287]\n",
      "loss: 0.000044  [22336/31287]\n",
      "loss: 0.029463  [22400/31287]\n",
      "loss: 0.002712  [22464/31287]\n",
      "loss: 0.000689  [22528/31287]\n",
      "loss: 0.000722  [22592/31287]\n",
      "loss: 0.000042  [22656/31287]\n",
      "loss: 0.000015  [22720/31287]\n",
      "loss: 0.000053  [22784/31287]\n",
      "loss: 0.000081  [22848/31287]\n",
      "loss: 0.000747  [22912/31287]\n",
      "loss: 0.026225  [22976/31287]\n",
      "loss: 0.000209  [23040/31287]\n",
      "loss: 0.005579  [23104/31287]\n",
      "loss: 0.000209  [23168/31287]\n",
      "loss: 0.001883  [23232/31287]\n",
      "loss: 0.002660  [23296/31287]\n",
      "loss: 0.000108  [23360/31287]\n",
      "loss: 0.000023  [23424/31287]\n",
      "loss: 0.000126  [23488/31287]\n",
      "loss: 0.000034  [23552/31287]\n",
      "loss: 0.000263  [23616/31287]\n",
      "loss: 0.000168  [23680/31287]\n",
      "loss: 0.004710  [23744/31287]\n",
      "loss: 0.018697  [23808/31287]\n",
      "loss: 0.000065  [23872/31287]\n",
      "loss: 0.001230  [23936/31287]\n",
      "loss: 0.000078  [24000/31287]\n",
      "loss: 0.006948  [24064/31287]\n",
      "loss: 0.000127  [24128/31287]\n",
      "loss: 0.154357  [24192/31287]\n",
      "loss: 0.000914  [24256/31287]\n",
      "loss: 0.000248  [24320/31287]\n",
      "loss: 0.000250  [24384/31287]\n",
      "loss: 0.000176  [24448/31287]\n",
      "loss: 0.006259  [24512/31287]\n",
      "loss: 0.007549  [24576/31287]\n",
      "loss: 0.000846  [24640/31287]\n",
      "loss: 0.196781  [24704/31287]\n",
      "loss: 0.023244  [24768/31287]\n",
      "loss: 0.001551  [24832/31287]\n",
      "loss: 0.004695  [24896/31287]\n",
      "loss: 0.000913  [24960/31287]\n",
      "loss: 0.010611  [25024/31287]\n",
      "loss: 0.003380  [25088/31287]\n",
      "loss: 0.001094  [25152/31287]\n",
      "loss: 0.013054  [25216/31287]\n",
      "loss: 0.006651  [25280/31287]\n",
      "loss: 0.010231  [25344/31287]\n",
      "loss: 0.002709  [25408/31287]\n",
      "loss: 0.000721  [25472/31287]\n",
      "loss: 0.000423  [25536/31287]\n",
      "loss: 0.000217  [25600/31287]\n",
      "loss: 0.002771  [25664/31287]\n",
      "loss: 0.003605  [25728/31287]\n",
      "loss: 0.001454  [25792/31287]\n",
      "loss: 0.000647  [25856/31287]\n",
      "loss: 0.004722  [25920/31287]\n",
      "loss: 0.005754  [25984/31287]\n",
      "loss: 0.000072  [26048/31287]\n",
      "loss: 0.000571  [26112/31287]\n",
      "loss: 0.000189  [26176/31287]\n",
      "loss: 0.001017  [26240/31287]\n",
      "loss: 0.000877  [26304/31287]\n",
      "loss: 0.000526  [26368/31287]\n",
      "loss: 0.001473  [26432/31287]\n",
      "loss: 0.000373  [26496/31287]\n",
      "loss: 0.000435  [26560/31287]\n",
      "loss: 0.000561  [26624/31287]\n",
      "loss: 0.045745  [26688/31287]\n",
      "loss: 0.000094  [26752/31287]\n",
      "loss: 0.000224  [26816/31287]\n",
      "loss: 0.000398  [26880/31287]\n",
      "loss: 0.271475  [26944/31287]\n",
      "loss: 0.238949  [27008/31287]\n",
      "loss: 0.019176  [27072/31287]\n",
      "loss: 0.002107  [27136/31287]\n",
      "loss: 0.014633  [27200/31287]\n",
      "loss: 0.092128  [27264/31287]\n",
      "loss: 0.008209  [27328/31287]\n",
      "loss: 0.009321  [27392/31287]\n",
      "loss: 0.001839  [27456/31287]\n",
      "loss: 0.001094  [27520/31287]\n",
      "loss: 0.001838  [27584/31287]\n",
      "loss: 0.000587  [27648/31287]\n",
      "loss: 0.116490  [27712/31287]\n",
      "loss: 0.000038  [27776/31287]\n",
      "loss: 0.000741  [27840/31287]\n",
      "loss: 0.003911  [27904/31287]\n",
      "loss: 0.000841  [27968/31287]\n",
      "loss: 0.003437  [28032/31287]\n",
      "loss: 0.001923  [28096/31287]\n",
      "loss: 0.000194  [28160/31287]\n",
      "loss: 0.000379  [28224/31287]\n",
      "loss: 0.000072  [28288/31287]\n",
      "loss: 0.001012  [28352/31287]\n",
      "loss: 0.000052  [28416/31287]\n",
      "loss: 0.000114  [28480/31287]\n",
      "loss: 0.000859  [28544/31287]\n",
      "loss: 0.029602  [28608/31287]\n",
      "loss: 0.000747  [28672/31287]\n",
      "loss: 0.002053  [28736/31287]\n",
      "loss: 0.038914  [28800/31287]\n",
      "loss: 0.001166  [28864/31287]\n",
      "loss: 0.000343  [28928/31287]\n",
      "loss: 0.000062  [28992/31287]\n",
      "loss: 0.000119  [29056/31287]\n",
      "loss: 0.005381  [29120/31287]\n",
      "loss: 0.008488  [29184/31287]\n",
      "loss: 0.002242  [29248/31287]\n",
      "loss: 0.022963  [29312/31287]\n",
      "loss: 0.000024  [29376/31287]\n",
      "loss: 0.000630  [29440/31287]\n",
      "loss: 0.000218  [29504/31287]\n",
      "loss: 0.000085  [29568/31287]\n",
      "loss: 0.000425  [29632/31287]\n",
      "loss: 0.000066  [29696/31287]\n",
      "loss: 0.000820  [29760/31287]\n",
      "loss: 0.003450  [29824/31287]\n",
      "loss: 0.004034  [29888/31287]\n",
      "loss: 0.003363  [29952/31287]\n",
      "loss: 0.000269  [30016/31287]\n",
      "loss: 0.079351  [30080/31287]\n",
      "loss: 0.004288  [30144/31287]\n",
      "loss: 0.003274  [30208/31287]\n",
      "loss: 0.042829  [30272/31287]\n",
      "loss: 0.000689  [30336/31287]\n",
      "loss: 0.004445  [30400/31287]\n",
      "loss: 0.001094  [30464/31287]\n",
      "loss: 0.008448  [30528/31287]\n",
      "loss: 0.000958  [30592/31287]\n",
      "loss: 0.056570  [30656/31287]\n",
      "loss: 0.046981  [30720/31287]\n",
      "loss: 0.003447  [30784/31287]\n",
      "loss: 0.000472  [30848/31287]\n",
      "loss: 0.002808  [30912/31287]\n",
      "loss: 0.003601  [30976/31287]\n",
      "loss: 0.000111  [31040/31287]\n",
      "loss: 0.001682  [31104/31287]\n",
      "loss: 0.003074  [31168/31287]\n",
      "loss: 0.001149  [31232/31287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.87%\n",
      "Precision: 0.22\n",
      "Recall: 0.47\n",
      "F1 Score: 0.30\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000641  [    0/31287]\n",
      "loss: 0.000316  [   64/31287]\n",
      "loss: 0.083037  [  128/31287]\n",
      "loss: 0.000147  [  192/31287]\n",
      "loss: 0.000741  [  256/31287]\n",
      "loss: 0.000809  [  320/31287]\n",
      "loss: 0.013572  [  384/31287]\n",
      "loss: 0.008545  [  448/31287]\n",
      "loss: 0.008540  [  512/31287]\n",
      "loss: 0.001123  [  576/31287]\n",
      "loss: 0.005204  [  640/31287]\n",
      "loss: 0.006566  [  704/31287]\n",
      "loss: 0.045205  [  768/31287]\n",
      "loss: 0.000107  [  832/31287]\n",
      "loss: 0.000509  [  896/31287]\n",
      "loss: 0.004456  [  960/31287]\n",
      "loss: 0.003354  [ 1024/31287]\n",
      "loss: 0.002345  [ 1088/31287]\n",
      "loss: 0.001106  [ 1152/31287]\n",
      "loss: 0.000268  [ 1216/31287]\n",
      "loss: 0.002412  [ 1280/31287]\n",
      "loss: 0.000423  [ 1344/31287]\n",
      "loss: 0.000370  [ 1408/31287]\n",
      "loss: 0.002466  [ 1472/31287]\n",
      "loss: 0.001559  [ 1536/31287]\n",
      "loss: 0.001004  [ 1600/31287]\n",
      "loss: 0.001099  [ 1664/31287]\n",
      "loss: 0.000907  [ 1728/31287]\n",
      "loss: 0.000526  [ 1792/31287]\n",
      "loss: 0.000175  [ 1856/31287]\n",
      "loss: 0.000058  [ 1920/31287]\n",
      "loss: 0.000124  [ 1984/31287]\n",
      "loss: 0.000121  [ 2048/31287]\n",
      "loss: 0.001909  [ 2112/31287]\n",
      "loss: 0.000096  [ 2176/31287]\n",
      "loss: 0.000428  [ 2240/31287]\n",
      "loss: 0.001185  [ 2304/31287]\n",
      "loss: 0.000088  [ 2368/31287]\n",
      "loss: 0.000428  [ 2432/31287]\n",
      "loss: 0.000071  [ 2496/31287]\n",
      "loss: 0.000708  [ 2560/31287]\n",
      "loss: 0.000117  [ 2624/31287]\n",
      "loss: 0.000072  [ 2688/31287]\n",
      "loss: 0.001322  [ 2752/31287]\n",
      "loss: 0.000356  [ 2816/31287]\n",
      "loss: 0.000418  [ 2880/31287]\n",
      "loss: 0.000061  [ 2944/31287]\n",
      "loss: 0.000905  [ 3008/31287]\n",
      "loss: 0.000025  [ 3072/31287]\n",
      "loss: 0.000875  [ 3136/31287]\n",
      "loss: 0.000032  [ 3200/31287]\n",
      "loss: 0.000141  [ 3264/31287]\n",
      "loss: 0.000029  [ 3328/31287]\n",
      "loss: 0.000012  [ 3392/31287]\n",
      "loss: 0.000324  [ 3456/31287]\n",
      "loss: 0.005027  [ 3520/31287]\n",
      "loss: 0.000257  [ 3584/31287]\n",
      "loss: 0.000037  [ 3648/31287]\n",
      "loss: 0.000049  [ 3712/31287]\n",
      "loss: 0.002646  [ 3776/31287]\n",
      "loss: 0.000001  [ 3840/31287]\n",
      "loss: 0.000181  [ 3904/31287]\n",
      "loss: 0.000005  [ 3968/31287]\n",
      "loss: 0.000158  [ 4032/31287]\n",
      "loss: 0.000355  [ 4096/31287]\n",
      "loss: 0.000017  [ 4160/31287]\n",
      "loss: 0.000025  [ 4224/31287]\n",
      "loss: 0.000153  [ 4288/31287]\n",
      "loss: 0.000009  [ 4352/31287]\n",
      "loss: 0.000045  [ 4416/31287]\n",
      "loss: 0.000009  [ 4480/31287]\n",
      "loss: 0.000007  [ 4544/31287]\n",
      "loss: 0.000002  [ 4608/31287]\n",
      "loss: 0.000585  [ 4672/31287]\n",
      "loss: 0.000375  [ 4736/31287]\n",
      "loss: 0.000027  [ 4800/31287]\n",
      "loss: 0.000188  [ 4864/31287]\n",
      "loss: 0.000005  [ 4928/31287]\n",
      "loss: 0.000004  [ 4992/31287]\n",
      "loss: 0.000013  [ 5056/31287]\n",
      "loss: 0.000092  [ 5120/31287]\n",
      "loss: 0.000017  [ 5184/31287]\n",
      "loss: 0.000054  [ 5248/31287]\n",
      "loss: 0.001944  [ 5312/31287]\n",
      "loss: 0.000157  [ 5376/31287]\n",
      "loss: 0.051224  [ 5440/31287]\n",
      "loss: 0.000014  [ 5504/31287]\n",
      "loss: 0.000004  [ 5568/31287]\n",
      "loss: 0.000159  [ 5632/31287]\n",
      "loss: 0.000006  [ 5696/31287]\n",
      "loss: 0.000014  [ 5760/31287]\n",
      "loss: 0.262641  [ 5824/31287]\n",
      "loss: 0.000458  [ 5888/31287]\n",
      "loss: 0.000011  [ 5952/31287]\n",
      "loss: 0.000015  [ 6016/31287]\n",
      "loss: 0.000706  [ 6080/31287]\n",
      "loss: 0.000717  [ 6144/31287]\n",
      "loss: 0.000079  [ 6208/31287]\n",
      "loss: 0.004821  [ 6272/31287]\n",
      "loss: 0.232434  [ 6336/31287]\n",
      "loss: 0.000229  [ 6400/31287]\n",
      "loss: 0.001527  [ 6464/31287]\n",
      "loss: 0.000802  [ 6528/31287]\n",
      "loss: 0.001558  [ 6592/31287]\n",
      "loss: 0.072228  [ 6656/31287]\n",
      "loss: 0.000424  [ 6720/31287]\n",
      "loss: 0.007126  [ 6784/31287]\n",
      "loss: 0.001661  [ 6848/31287]\n",
      "loss: 0.006842  [ 6912/31287]\n",
      "loss: 0.003752  [ 6976/31287]\n",
      "loss: 0.000896  [ 7040/31287]\n",
      "loss: 0.005310  [ 7104/31287]\n",
      "loss: 0.000583  [ 7168/31287]\n",
      "loss: 0.002285  [ 7232/31287]\n",
      "loss: 0.000330  [ 7296/31287]\n",
      "loss: 0.000237  [ 7360/31287]\n",
      "loss: 0.000160  [ 7424/31287]\n",
      "loss: 0.002084  [ 7488/31287]\n",
      "loss: 0.003254  [ 7552/31287]\n",
      "loss: 0.000465  [ 7616/31287]\n",
      "loss: 0.000617  [ 7680/31287]\n",
      "loss: 0.000440  [ 7744/31287]\n",
      "loss: 0.000648  [ 7808/31287]\n",
      "loss: 0.134839  [ 7872/31287]\n",
      "loss: 0.000414  [ 7936/31287]\n",
      "loss: 0.001458  [ 8000/31287]\n",
      "loss: 0.000445  [ 8064/31287]\n",
      "loss: 0.000270  [ 8128/31287]\n",
      "loss: 0.001506  [ 8192/31287]\n",
      "loss: 0.001201  [ 8256/31287]\n",
      "loss: 0.077018  [ 8320/31287]\n",
      "loss: 0.001514  [ 8384/31287]\n",
      "loss: 0.004949  [ 8448/31287]\n",
      "loss: 0.082284  [ 8512/31287]\n",
      "loss: 0.000450  [ 8576/31287]\n",
      "loss: 0.073429  [ 8640/31287]\n",
      "loss: 0.001049  [ 8704/31287]\n",
      "loss: 0.010404  [ 8768/31287]\n",
      "loss: 0.002547  [ 8832/31287]\n",
      "loss: 0.004505  [ 8896/31287]\n",
      "loss: 0.002768  [ 8960/31287]\n",
      "loss: 0.000211  [ 9024/31287]\n",
      "loss: 0.035385  [ 9088/31287]\n",
      "loss: 0.003602  [ 9152/31287]\n",
      "loss: 0.000255  [ 9216/31287]\n",
      "loss: 0.007722  [ 9280/31287]\n",
      "loss: 0.076230  [ 9344/31287]\n",
      "loss: 0.006095  [ 9408/31287]\n",
      "loss: 0.000674  [ 9472/31287]\n",
      "loss: 0.000893  [ 9536/31287]\n",
      "loss: 0.129721  [ 9600/31287]\n",
      "loss: 0.001710  [ 9664/31287]\n",
      "loss: 0.003810  [ 9728/31287]\n",
      "loss: 0.000492  [ 9792/31287]\n",
      "loss: 0.001049  [ 9856/31287]\n",
      "loss: 0.007003  [ 9920/31287]\n",
      "loss: 0.000355  [ 9984/31287]\n",
      "loss: 0.000696  [10048/31287]\n",
      "loss: 0.000177  [10112/31287]\n",
      "loss: 0.001394  [10176/31287]\n",
      "loss: 0.009343  [10240/31287]\n",
      "loss: 0.002072  [10304/31287]\n",
      "loss: 0.000270  [10368/31287]\n",
      "loss: 0.000563  [10432/31287]\n",
      "loss: 0.000189  [10496/31287]\n",
      "loss: 0.006070  [10560/31287]\n",
      "loss: 0.000598  [10624/31287]\n",
      "loss: 0.002078  [10688/31287]\n",
      "loss: 0.000225  [10752/31287]\n",
      "loss: 0.006834  [10816/31287]\n",
      "loss: 0.000215  [10880/31287]\n",
      "loss: 0.015336  [10944/31287]\n",
      "loss: 0.000128  [11008/31287]\n",
      "loss: 0.001011  [11072/31287]\n",
      "loss: 0.000701  [11136/31287]\n",
      "loss: 0.000334  [11200/31287]\n",
      "loss: 0.001839  [11264/31287]\n",
      "loss: 0.018994  [11328/31287]\n",
      "loss: 0.002618  [11392/31287]\n",
      "loss: 0.081787  [11456/31287]\n",
      "loss: 0.033253  [11520/31287]\n",
      "loss: 0.000267  [11584/31287]\n",
      "loss: 0.000071  [11648/31287]\n",
      "loss: 0.000531  [11712/31287]\n",
      "loss: 0.000078  [11776/31287]\n",
      "loss: 0.010244  [11840/31287]\n",
      "loss: 0.069229  [11904/31287]\n",
      "loss: 0.000815  [11968/31287]\n",
      "loss: 0.000249  [12032/31287]\n",
      "loss: 0.000373  [12096/31287]\n",
      "loss: 0.005964  [12160/31287]\n",
      "loss: 0.017083  [12224/31287]\n",
      "loss: 0.001955  [12288/31287]\n",
      "loss: 0.000085  [12352/31287]\n",
      "loss: 0.003214  [12416/31287]\n",
      "loss: 0.000170  [12480/31287]\n",
      "loss: 0.001123  [12544/31287]\n",
      "loss: 0.000330  [12608/31287]\n",
      "loss: 0.000581  [12672/31287]\n",
      "loss: 0.000075  [12736/31287]\n",
      "loss: 0.001913  [12800/31287]\n",
      "loss: 0.010896  [12864/31287]\n",
      "loss: 0.000569  [12928/31287]\n",
      "loss: 0.122920  [12992/31287]\n",
      "loss: 0.000094  [13056/31287]\n",
      "loss: 0.001099  [13120/31287]\n",
      "loss: 0.000798  [13184/31287]\n",
      "loss: 0.001278  [13248/31287]\n",
      "loss: 0.000479  [13312/31287]\n",
      "loss: 0.002957  [13376/31287]\n",
      "loss: 0.028837  [13440/31287]\n",
      "loss: 0.021786  [13504/31287]\n",
      "loss: 0.003012  [13568/31287]\n",
      "loss: 0.003026  [13632/31287]\n",
      "loss: 0.003570  [13696/31287]\n",
      "loss: 0.001246  [13760/31287]\n",
      "loss: 0.044030  [13824/31287]\n",
      "loss: 0.003803  [13888/31287]\n",
      "loss: 0.000846  [13952/31287]\n",
      "loss: 0.000383  [14016/31287]\n",
      "loss: 0.004654  [14080/31287]\n",
      "loss: 0.005966  [14144/31287]\n",
      "loss: 0.000257  [14208/31287]\n",
      "loss: 0.002438  [14272/31287]\n",
      "loss: 0.003040  [14336/31287]\n",
      "loss: 0.000876  [14400/31287]\n",
      "loss: 0.000023  [14464/31287]\n",
      "loss: 0.021023  [14528/31287]\n",
      "loss: 0.000603  [14592/31287]\n",
      "loss: 0.005249  [14656/31287]\n",
      "loss: 0.000062  [14720/31287]\n",
      "loss: 0.000364  [14784/31287]\n",
      "loss: 0.000034  [14848/31287]\n",
      "loss: 0.000079  [14912/31287]\n",
      "loss: 0.000044  [14976/31287]\n",
      "loss: 0.000167  [15040/31287]\n",
      "loss: 0.000048  [15104/31287]\n",
      "loss: 0.000012  [15168/31287]\n",
      "loss: 0.000009  [15232/31287]\n",
      "loss: 0.000065  [15296/31287]\n",
      "loss: 0.000000  [15360/31287]\n",
      "loss: 0.000111  [15424/31287]\n",
      "loss: 0.000016  [15488/31287]\n",
      "loss: 0.000549  [15552/31287]\n",
      "loss: 0.000888  [15616/31287]\n",
      "loss: 0.000028  [15680/31287]\n",
      "loss: 0.000024  [15744/31287]\n",
      "loss: 0.000136  [15808/31287]\n",
      "loss: 0.000108  [15872/31287]\n",
      "loss: 0.002223  [15936/31287]\n",
      "loss: 0.000010  [16000/31287]\n",
      "loss: 0.002548  [16064/31287]\n",
      "loss: 0.000196  [16128/31287]\n",
      "loss: 0.000644  [16192/31287]\n",
      "loss: 0.000331  [16256/31287]\n",
      "loss: 0.001059  [16320/31287]\n",
      "loss: 0.007701  [16384/31287]\n",
      "loss: 0.011643  [16448/31287]\n",
      "loss: 0.000500  [16512/31287]\n",
      "loss: 0.000623  [16576/31287]\n",
      "loss: 0.000182  [16640/31287]\n",
      "loss: 0.002231  [16704/31287]\n",
      "loss: 0.002184  [16768/31287]\n",
      "loss: 0.000822  [16832/31287]\n",
      "loss: 0.000255  [16896/31287]\n",
      "loss: 0.057520  [16960/31287]\n",
      "loss: 0.000458  [17024/31287]\n",
      "loss: 0.000284  [17088/31287]\n",
      "loss: 0.000106  [17152/31287]\n",
      "loss: 0.001122  [17216/31287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.010729  [17280/31287]\n",
      "loss: 0.000394  [17344/31287]\n",
      "loss: 0.005205  [17408/31287]\n",
      "loss: 0.004382  [17472/31287]\n",
      "loss: 0.001827  [17536/31287]\n",
      "loss: 0.001479  [17600/31287]\n",
      "loss: 0.003907  [17664/31287]\n",
      "loss: 0.000483  [17728/31287]\n",
      "loss: 0.000257  [17792/31287]\n",
      "loss: 0.001228  [17856/31287]\n",
      "loss: 0.000166  [17920/31287]\n",
      "loss: 0.002346  [17984/31287]\n",
      "loss: 0.002723  [18048/31287]\n",
      "loss: 0.000774  [18112/31287]\n",
      "loss: 0.001234  [18176/31287]\n",
      "loss: 0.000270  [18240/31287]\n",
      "loss: 0.002536  [18304/31287]\n",
      "loss: 0.000221  [18368/31287]\n",
      "loss: 0.001234  [18432/31287]\n",
      "loss: 0.000251  [18496/31287]\n",
      "loss: 0.008725  [18560/31287]\n",
      "loss: 0.001103  [18624/31287]\n",
      "loss: 0.001455  [18688/31287]\n",
      "loss: 0.000823  [18752/31287]\n",
      "loss: 0.322279  [18816/31287]\n",
      "loss: 0.110380  [18880/31287]\n",
      "loss: 0.039942  [18944/31287]\n",
      "loss: 0.005448  [19008/31287]\n",
      "loss: 0.089497  [19072/31287]\n",
      "loss: 0.005984  [19136/31287]\n",
      "loss: 0.041105  [19200/31287]\n",
      "loss: 0.014046  [19264/31287]\n",
      "loss: 0.012997  [19328/31287]\n",
      "loss: 0.006181  [19392/31287]\n",
      "loss: 0.000590  [19456/31287]\n",
      "loss: 0.002430  [19520/31287]\n",
      "loss: 0.007123  [19584/31287]\n",
      "loss: 0.002449  [19648/31287]\n",
      "loss: 0.003622  [19712/31287]\n",
      "loss: 0.002666  [19776/31287]\n",
      "loss: 0.015068  [19840/31287]\n",
      "loss: 0.030871  [19904/31287]\n",
      "loss: 0.000683  [19968/31287]\n",
      "loss: 0.049060  [20032/31287]\n",
      "loss: 0.001808  [20096/31287]\n",
      "loss: 0.038119  [20160/31287]\n",
      "loss: 0.000850  [20224/31287]\n",
      "loss: 0.032396  [20288/31287]\n",
      "loss: 0.015211  [20352/31287]\n",
      "loss: 0.014496  [20416/31287]\n",
      "loss: 0.000021  [20480/31287]\n",
      "loss: 0.000031  [20544/31287]\n",
      "loss: 0.000987  [20608/31287]\n",
      "loss: 0.000539  [20672/31287]\n",
      "loss: 0.000104  [20736/31287]\n",
      "loss: 0.001029  [20800/31287]\n",
      "loss: 0.001236  [20864/31287]\n",
      "loss: 0.000351  [20928/31287]\n",
      "loss: 0.059826  [20992/31287]\n",
      "loss: 0.003699  [21056/31287]\n",
      "loss: 0.000733  [21120/31287]\n",
      "loss: 0.001878  [21184/31287]\n",
      "loss: 0.016679  [21248/31287]\n",
      "loss: 0.001033  [21312/31287]\n",
      "loss: 0.001647  [21376/31287]\n",
      "loss: 0.001028  [21440/31287]\n",
      "loss: 0.001714  [21504/31287]\n",
      "loss: 0.001036  [21568/31287]\n",
      "loss: 0.000148  [21632/31287]\n",
      "loss: 0.000442  [21696/31287]\n",
      "loss: 0.000233  [21760/31287]\n",
      "loss: 0.000236  [21824/31287]\n",
      "loss: 0.000570  [21888/31287]\n",
      "loss: 0.053160  [21952/31287]\n",
      "loss: 0.001833  [22016/31287]\n",
      "loss: 0.000636  [22080/31287]\n",
      "loss: 0.033659  [22144/31287]\n",
      "loss: 0.005262  [22208/31287]\n",
      "loss: 0.111736  [22272/31287]\n",
      "loss: 0.000747  [22336/31287]\n",
      "loss: 0.000937  [22400/31287]\n",
      "loss: 0.000189  [22464/31287]\n",
      "loss: 0.000608  [22528/31287]\n",
      "loss: 0.000081  [22592/31287]\n",
      "loss: 0.000412  [22656/31287]\n",
      "loss: 0.001015  [22720/31287]\n",
      "loss: 0.000136  [22784/31287]\n",
      "loss: 0.000002  [22848/31287]\n",
      "loss: 0.000428  [22912/31287]\n",
      "loss: 0.000030  [22976/31287]\n",
      "loss: 0.002926  [23040/31287]\n",
      "loss: 0.000422  [23104/31287]\n",
      "loss: 0.002266  [23168/31287]\n",
      "loss: 0.000333  [23232/31287]\n",
      "loss: 0.000166  [23296/31287]\n",
      "loss: 0.000435  [23360/31287]\n",
      "loss: 0.000080  [23424/31287]\n",
      "loss: 0.000012  [23488/31287]\n",
      "loss: 0.000076  [23552/31287]\n",
      "loss: 0.017602  [23616/31287]\n",
      "loss: 0.263281  [23680/31287]\n",
      "loss: 0.000380  [23744/31287]\n",
      "loss: 0.034570  [23808/31287]\n",
      "loss: 0.039016  [23872/31287]\n",
      "loss: 0.004211  [23936/31287]\n",
      "loss: 0.000655  [24000/31287]\n",
      "loss: 0.008320  [24064/31287]\n",
      "loss: 0.003955  [24128/31287]\n",
      "loss: 0.025906  [24192/31287]\n",
      "loss: 0.009838  [24256/31287]\n",
      "loss: 0.160168  [24320/31287]\n",
      "loss: 0.013305  [24384/31287]\n",
      "loss: 0.008058  [24448/31287]\n",
      "loss: 0.007969  [24512/31287]\n",
      "loss: 0.010851  [24576/31287]\n",
      "loss: 0.001646  [24640/31287]\n",
      "loss: 0.004707  [24704/31287]\n",
      "loss: 0.000064  [24768/31287]\n",
      "loss: 0.001756  [24832/31287]\n",
      "loss: 0.000680  [24896/31287]\n",
      "loss: 0.000018  [24960/31287]\n",
      "loss: 0.236354  [25024/31287]\n",
      "loss: 0.000195  [25088/31287]\n",
      "loss: 0.000117  [25152/31287]\n",
      "loss: 0.001564  [25216/31287]\n",
      "loss: 0.000911  [25280/31287]\n",
      "loss: 0.003401  [25344/31287]\n",
      "loss: 0.003762  [25408/31287]\n",
      "loss: 0.000635  [25472/31287]\n",
      "loss: 0.000459  [25536/31287]\n",
      "loss: 0.000257  [25600/31287]\n",
      "loss: 0.002854  [25664/31287]\n",
      "loss: 0.002583  [25728/31287]\n",
      "loss: 0.000431  [25792/31287]\n",
      "loss: 0.000249  [25856/31287]\n",
      "loss: 0.000279  [25920/31287]\n",
      "loss: 0.000694  [25984/31287]\n",
      "loss: 0.000441  [26048/31287]\n",
      "loss: 0.000846  [26112/31287]\n",
      "loss: 0.002312  [26176/31287]\n",
      "loss: 0.000989  [26240/31287]\n",
      "loss: 0.001426  [26304/31287]\n",
      "loss: 0.000134  [26368/31287]\n",
      "loss: 0.000288  [26432/31287]\n",
      "loss: 0.000395  [26496/31287]\n",
      "loss: 0.000987  [26560/31287]\n",
      "loss: 0.001704  [26624/31287]\n",
      "loss: 0.002877  [26688/31287]\n",
      "loss: 0.037868  [26752/31287]\n",
      "loss: 0.000059  [26816/31287]\n",
      "loss: 0.000140  [26880/31287]\n",
      "loss: 0.000035  [26944/31287]\n",
      "loss: 0.000030  [27008/31287]\n",
      "loss: 0.000056  [27072/31287]\n",
      "loss: 0.000029  [27136/31287]\n",
      "loss: 0.056098  [27200/31287]\n",
      "loss: 0.073939  [27264/31287]\n",
      "loss: 0.000027  [27328/31287]\n",
      "loss: 0.000045  [27392/31287]\n",
      "loss: 0.000226  [27456/31287]\n",
      "loss: 0.000031  [27520/31287]\n",
      "loss: 0.000206  [27584/31287]\n",
      "loss: 0.000185  [27648/31287]\n",
      "loss: 0.001889  [27712/31287]\n",
      "loss: 0.000019  [27776/31287]\n",
      "loss: 0.000055  [27840/31287]\n",
      "loss: 0.000127  [27904/31287]\n",
      "loss: 0.000125  [27968/31287]\n",
      "loss: 0.000395  [28032/31287]\n",
      "loss: 0.003579  [28096/31287]\n",
      "loss: 0.000081  [28160/31287]\n",
      "loss: 0.000046  [28224/31287]\n",
      "loss: 0.000018  [28288/31287]\n",
      "loss: 0.000289  [28352/31287]\n",
      "loss: 0.000075  [28416/31287]\n",
      "loss: 0.000019  [28480/31287]\n",
      "loss: 0.000178  [28544/31287]\n",
      "loss: 0.085626  [28608/31287]\n",
      "loss: 0.000499  [28672/31287]\n",
      "loss: 0.000619  [28736/31287]\n",
      "loss: 0.000523  [28800/31287]\n",
      "loss: 0.000572  [28864/31287]\n",
      "loss: 0.003857  [28928/31287]\n",
      "loss: 0.000006  [28992/31287]\n",
      "loss: 0.000421  [29056/31287]\n",
      "loss: 0.000118  [29120/31287]\n",
      "loss: 0.005568  [29184/31287]\n",
      "loss: 0.000251  [29248/31287]\n",
      "loss: 0.000351  [29312/31287]\n",
      "loss: 0.000070  [29376/31287]\n",
      "loss: 0.000107  [29440/31287]\n",
      "loss: 0.000027  [29504/31287]\n",
      "loss: 0.000248  [29568/31287]\n",
      "loss: 0.000237  [29632/31287]\n",
      "loss: 0.025153  [29696/31287]\n",
      "loss: 0.058594  [29760/31287]\n",
      "loss: 0.010613  [29824/31287]\n",
      "loss: 0.012358  [29888/31287]\n",
      "loss: 0.000233  [29952/31287]\n",
      "loss: 0.000274  [30016/31287]\n",
      "loss: 0.002653  [30080/31287]\n",
      "loss: 0.021032  [30144/31287]\n",
      "loss: 0.016791  [30208/31287]\n",
      "loss: 0.000703  [30272/31287]\n",
      "loss: 0.000280  [30336/31287]\n",
      "loss: 0.005781  [30400/31287]\n",
      "loss: 0.001246  [30464/31287]\n",
      "loss: 0.003988  [30528/31287]\n",
      "loss: 0.001397  [30592/31287]\n",
      "loss: 0.040909  [30656/31287]\n",
      "loss: 0.001620  [30720/31287]\n",
      "loss: 0.000709  [30784/31287]\n",
      "loss: 0.001962  [30848/31287]\n",
      "loss: 0.003805  [30912/31287]\n",
      "loss: 0.004449  [30976/31287]\n",
      "loss: 0.000360  [31040/31287]\n",
      "loss: 0.130995  [31104/31287]\n",
      "loss: 0.024121  [31168/31287]\n",
      "loss: 0.011671  [31232/31287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.87%\n",
      "Precision: 0.22\n",
      "Recall: 0.47\n",
      "F1 Score: 0.30\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000410  [    0/31287]\n",
      "loss: 0.000098  [   64/31287]\n",
      "loss: 0.000911  [  128/31287]\n",
      "loss: 0.000163  [  192/31287]\n",
      "loss: 0.001111  [  256/31287]\n",
      "loss: 0.003746  [  320/31287]\n",
      "loss: 0.001711  [  384/31287]\n",
      "loss: 0.031214  [  448/31287]\n",
      "loss: 0.002266  [  512/31287]\n",
      "loss: 0.000245  [  576/31287]\n",
      "loss: 0.040403  [  640/31287]\n",
      "loss: 0.042897  [  704/31287]\n",
      "loss: 0.176851  [  768/31287]\n",
      "loss: 0.000136  [  832/31287]\n",
      "loss: 0.002348  [  896/31287]\n",
      "loss: 0.005035  [  960/31287]\n",
      "loss: 0.002584  [ 1024/31287]\n",
      "loss: 0.003902  [ 1088/31287]\n",
      "loss: 0.023934  [ 1152/31287]\n",
      "loss: 0.002440  [ 1216/31287]\n",
      "loss: 0.002845  [ 1280/31287]\n",
      "loss: 0.012220  [ 1344/31287]\n",
      "loss: 0.001473  [ 1408/31287]\n",
      "loss: 0.000999  [ 1472/31287]\n",
      "loss: 0.000844  [ 1536/31287]\n",
      "loss: 0.001561  [ 1600/31287]\n",
      "loss: 0.001920  [ 1664/31287]\n",
      "loss: 0.000966  [ 1728/31287]\n",
      "loss: 0.000897  [ 1792/31287]\n",
      "loss: 0.000587  [ 1856/31287]\n",
      "loss: 0.000165  [ 1920/31287]\n",
      "loss: 0.001424  [ 1984/31287]\n",
      "loss: 0.000093  [ 2048/31287]\n",
      "loss: 0.000679  [ 2112/31287]\n",
      "loss: 0.000250  [ 2176/31287]\n",
      "loss: 0.085533  [ 2240/31287]\n",
      "loss: 0.004965  [ 2304/31287]\n",
      "loss: 0.010898  [ 2368/31287]\n",
      "loss: 0.007810  [ 2432/31287]\n",
      "loss: 0.000332  [ 2496/31287]\n",
      "loss: 0.000103  [ 2560/31287]\n",
      "loss: 0.003733  [ 2624/31287]\n",
      "loss: 0.000050  [ 2688/31287]\n",
      "loss: 0.000622  [ 2752/31287]\n",
      "loss: 0.000571  [ 2816/31287]\n",
      "loss: 0.000227  [ 2880/31287]\n",
      "loss: 0.000150  [ 2944/31287]\n",
      "loss: 0.000250  [ 3008/31287]\n",
      "loss: 0.004829  [ 3072/31287]\n",
      "loss: 0.001259  [ 3136/31287]\n",
      "loss: 0.000237  [ 3200/31287]\n",
      "loss: 0.000062  [ 3264/31287]\n",
      "loss: 0.000021  [ 3328/31287]\n",
      "loss: 0.000113  [ 3392/31287]\n",
      "loss: 0.000092  [ 3456/31287]\n",
      "loss: 0.000484  [ 3520/31287]\n",
      "loss: 0.000091  [ 3584/31287]\n",
      "loss: 0.000147  [ 3648/31287]\n",
      "loss: 0.000248  [ 3712/31287]\n",
      "loss: 0.000044  [ 3776/31287]\n",
      "loss: 0.000027  [ 3840/31287]\n",
      "loss: 0.000107  [ 3904/31287]\n",
      "loss: 0.000008  [ 3968/31287]\n",
      "loss: 0.068072  [ 4032/31287]\n",
      "loss: 0.000158  [ 4096/31287]\n",
      "loss: 0.000027  [ 4160/31287]\n",
      "loss: 0.000054  [ 4224/31287]\n",
      "loss: 0.026437  [ 4288/31287]\n",
      "loss: 0.000031  [ 4352/31287]\n",
      "loss: 0.000152  [ 4416/31287]\n",
      "loss: 0.000038  [ 4480/31287]\n",
      "loss: 0.000005  [ 4544/31287]\n",
      "loss: 0.000431  [ 4608/31287]\n",
      "loss: 0.004469  [ 4672/31287]\n",
      "loss: 0.000671  [ 4736/31287]\n",
      "loss: 0.000459  [ 4800/31287]\n",
      "loss: 0.001819  [ 4864/31287]\n",
      "loss: 0.000035  [ 4928/31287]\n",
      "loss: 0.000044  [ 4992/31287]\n",
      "loss: 0.001410  [ 5056/31287]\n",
      "loss: 0.022399  [ 5120/31287]\n",
      "loss: 0.000806  [ 5184/31287]\n",
      "loss: 0.014615  [ 5248/31287]\n",
      "loss: 0.000330  [ 5312/31287]\n",
      "loss: 0.000489  [ 5376/31287]\n",
      "loss: 0.000584  [ 5440/31287]\n",
      "loss: 0.000209  [ 5504/31287]\n",
      "loss: 0.000252  [ 5568/31287]\n",
      "loss: 0.000454  [ 5632/31287]\n",
      "loss: 0.000157  [ 5696/31287]\n",
      "loss: 0.009046  [ 5760/31287]\n",
      "loss: 0.000193  [ 5824/31287]\n",
      "loss: 0.000072  [ 5888/31287]\n",
      "loss: 0.000207  [ 5952/31287]\n",
      "loss: 0.000373  [ 6016/31287]\n",
      "loss: 0.002614  [ 6080/31287]\n",
      "loss: 0.000149  [ 6144/31287]\n",
      "loss: 0.000780  [ 6208/31287]\n",
      "loss: 0.000067  [ 6272/31287]\n",
      "loss: 0.023202  [ 6336/31287]\n",
      "loss: 0.000003  [ 6400/31287]\n",
      "loss: 0.000101  [ 6464/31287]\n",
      "loss: 0.000085  [ 6528/31287]\n",
      "loss: 0.000279  [ 6592/31287]\n",
      "loss: 0.000451  [ 6656/31287]\n",
      "loss: 0.000121  [ 6720/31287]\n",
      "loss: 0.000115  [ 6784/31287]\n",
      "loss: 0.000065  [ 6848/31287]\n",
      "loss: 0.000022  [ 6912/31287]\n",
      "loss: 0.000025  [ 6976/31287]\n",
      "loss: 0.000243  [ 7040/31287]\n",
      "loss: 0.000016  [ 7104/31287]\n",
      "loss: 0.047984  [ 7168/31287]\n",
      "loss: 0.057537  [ 7232/31287]\n",
      "loss: 0.000008  [ 7296/31287]\n",
      "loss: 0.000027  [ 7360/31287]\n",
      "loss: 0.000095  [ 7424/31287]\n",
      "loss: 0.020398  [ 7488/31287]\n",
      "loss: 0.000015  [ 7552/31287]\n",
      "loss: 0.000216  [ 7616/31287]\n",
      "loss: 0.000065  [ 7680/31287]\n",
      "loss: 0.009333  [ 7744/31287]\n",
      "loss: 0.001463  [ 7808/31287]\n",
      "loss: 0.000283  [ 7872/31287]\n",
      "loss: 0.001163  [ 7936/31287]\n",
      "loss: 0.000095  [ 8000/31287]\n",
      "loss: 0.000028  [ 8064/31287]\n",
      "loss: 0.000077  [ 8128/31287]\n",
      "loss: 0.000033  [ 8192/31287]\n",
      "loss: 0.000090  [ 8256/31287]\n",
      "loss: 0.011772  [ 8320/31287]\n",
      "loss: 0.000196  [ 8384/31287]\n",
      "loss: 0.000048  [ 8448/31287]\n",
      "loss: 0.000005  [ 8512/31287]\n",
      "loss: 0.000238  [ 8576/31287]\n",
      "loss: 0.000794  [ 8640/31287]\n",
      "loss: 0.000043  [ 8704/31287]\n",
      "loss: 0.003851  [ 8768/31287]\n",
      "loss: 0.118123  [ 8832/31287]\n",
      "loss: 0.000031  [ 8896/31287]\n",
      "loss: 0.000113  [ 8960/31287]\n",
      "loss: 0.000263  [ 9024/31287]\n",
      "loss: 0.060688  [ 9088/31287]\n",
      "loss: 0.177480  [ 9152/31287]\n",
      "loss: 0.005446  [ 9216/31287]\n",
      "loss: 0.003210  [ 9280/31287]\n",
      "loss: 0.032283  [ 9344/31287]\n",
      "loss: 0.013291  [ 9408/31287]\n",
      "loss: 0.005878  [ 9472/31287]\n",
      "loss: 0.001961  [ 9536/31287]\n",
      "loss: 0.005824  [ 9600/31287]\n",
      "loss: 0.001706  [ 9664/31287]\n",
      "loss: 0.004026  [ 9728/31287]\n",
      "loss: 0.001518  [ 9792/31287]\n",
      "loss: 0.000260  [ 9856/31287]\n",
      "loss: 0.000828  [ 9920/31287]\n",
      "loss: 0.003768  [ 9984/31287]\n",
      "loss: 0.002007  [10048/31287]\n",
      "loss: 0.002672  [10112/31287]\n",
      "loss: 0.000807  [10176/31287]\n",
      "loss: 0.007175  [10240/31287]\n",
      "loss: 0.000262  [10304/31287]\n",
      "loss: 0.003102  [10368/31287]\n",
      "loss: 0.000021  [10432/31287]\n",
      "loss: 0.000024  [10496/31287]\n",
      "loss: 0.000035  [10560/31287]\n",
      "loss: 0.002694  [10624/31287]\n",
      "loss: 0.000011  [10688/31287]\n",
      "loss: 0.000795  [10752/31287]\n",
      "loss: 0.026851  [10816/31287]\n",
      "loss: 0.000008  [10880/31287]\n",
      "loss: 0.000021  [10944/31287]\n",
      "loss: 0.000001  [11008/31287]\n",
      "loss: 0.265000  [11072/31287]\n",
      "loss: 0.000210  [11136/31287]\n",
      "loss: 0.000065  [11200/31287]\n",
      "loss: 0.001333  [11264/31287]\n",
      "loss: 0.000213  [11328/31287]\n",
      "loss: 0.004507  [11392/31287]\n",
      "loss: 0.003424  [11456/31287]\n",
      "loss: 0.000719  [11520/31287]\n",
      "loss: 0.001455  [11584/31287]\n",
      "loss: 0.074573  [11648/31287]\n",
      "loss: 0.006992  [11712/31287]\n",
      "loss: 0.000118  [11776/31287]\n",
      "loss: 0.000595  [11840/31287]\n",
      "loss: 0.000180  [11904/31287]\n",
      "loss: 0.000131  [11968/31287]\n",
      "loss: 0.000278  [12032/31287]\n",
      "loss: 0.056262  [12096/31287]\n",
      "loss: 0.001775  [12160/31287]\n",
      "loss: 0.000049  [12224/31287]\n",
      "loss: 0.000428  [12288/31287]\n",
      "loss: 0.465069  [12352/31287]\n",
      "loss: 0.000042  [12416/31287]\n",
      "loss: 0.000066  [12480/31287]\n",
      "loss: 0.000109  [12544/31287]\n",
      "loss: 0.000364  [12608/31287]\n",
      "loss: 0.000799  [12672/31287]\n",
      "loss: 0.095439  [12736/31287]\n",
      "loss: 0.001509  [12800/31287]\n",
      "loss: 0.000512  [12864/31287]\n",
      "loss: 0.004788  [12928/31287]\n",
      "loss: 0.000456  [12992/31287]\n",
      "loss: 0.000339  [13056/31287]\n",
      "loss: 0.000237  [13120/31287]\n",
      "loss: 0.002599  [13184/31287]\n",
      "loss: 0.000467  [13248/31287]\n",
      "loss: 0.002275  [13312/31287]\n",
      "loss: 0.000863  [13376/31287]\n",
      "loss: 0.003666  [13440/31287]\n",
      "loss: 0.066088  [13504/31287]\n",
      "loss: 0.016002  [13568/31287]\n",
      "loss: 0.015081  [13632/31287]\n",
      "loss: 0.002906  [13696/31287]\n",
      "loss: 0.001124  [13760/31287]\n",
      "loss: 0.069608  [13824/31287]\n",
      "loss: 0.002625  [13888/31287]\n",
      "loss: 0.002714  [13952/31287]\n",
      "loss: 0.000031  [14016/31287]\n",
      "loss: 0.002477  [14080/31287]\n",
      "loss: 0.000012  [14144/31287]\n",
      "loss: 0.000731  [14208/31287]\n",
      "loss: 0.001172  [14272/31287]\n",
      "loss: 0.046319  [14336/31287]\n",
      "loss: 0.482078  [14400/31287]\n",
      "loss: 0.000143  [14464/31287]\n",
      "loss: 0.003650  [14528/31287]\n",
      "loss: 0.004709  [14592/31287]\n",
      "loss: 0.000740  [14656/31287]\n",
      "loss: 0.043101  [14720/31287]\n",
      "loss: 0.014457  [14784/31287]\n",
      "loss: 0.016340  [14848/31287]\n",
      "loss: 0.051370  [14912/31287]\n",
      "loss: 0.004657  [14976/31287]\n",
      "loss: 0.010508  [15040/31287]\n",
      "loss: 0.012482  [15104/31287]\n",
      "loss: 0.002279  [15168/31287]\n",
      "loss: 0.002716  [15232/31287]\n",
      "loss: 0.231026  [15296/31287]\n",
      "loss: 0.003213  [15360/31287]\n",
      "loss: 0.004551  [15424/31287]\n",
      "loss: 0.006892  [15488/31287]\n",
      "loss: 0.017169  [15552/31287]\n",
      "loss: 0.116807  [15616/31287]\n",
      "loss: 0.002988  [15680/31287]\n",
      "loss: 0.003712  [15744/31287]\n",
      "loss: 0.001933  [15808/31287]\n",
      "loss: 0.000076  [15872/31287]\n",
      "loss: 0.002243  [15936/31287]\n",
      "loss: 0.000059  [16000/31287]\n",
      "loss: 0.030010  [16064/31287]\n",
      "loss: 0.003403  [16128/31287]\n",
      "loss: 0.000503  [16192/31287]\n",
      "loss: 0.000051  [16256/31287]\n",
      "loss: 0.000584  [16320/31287]\n",
      "loss: 0.000041  [16384/31287]\n",
      "loss: 0.012473  [16448/31287]\n",
      "loss: 0.000047  [16512/31287]\n",
      "loss: 0.006340  [16576/31287]\n",
      "loss: 0.000003  [16640/31287]\n",
      "loss: 0.000018  [16704/31287]\n",
      "loss: 0.000011  [16768/31287]\n",
      "loss: 0.000127  [16832/31287]\n",
      "loss: 0.001603  [16896/31287]\n",
      "loss: 0.454594  [16960/31287]\n",
      "loss: 0.000857  [17024/31287]\n",
      "loss: 0.010079  [17088/31287]\n",
      "loss: 0.000233  [17152/31287]\n",
      "loss: 0.070514  [17216/31287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.057192  [17280/31287]\n",
      "loss: 0.001388  [17344/31287]\n",
      "loss: 0.005629  [17408/31287]\n",
      "loss: 0.009260  [17472/31287]\n",
      "loss: 0.000377  [17536/31287]\n",
      "loss: 0.053955  [17600/31287]\n",
      "loss: 0.000294  [17664/31287]\n",
      "loss: 0.000621  [17728/31287]\n",
      "loss: 0.000014  [17792/31287]\n",
      "loss: 0.000174  [17856/31287]\n",
      "loss: 0.000088  [17920/31287]\n",
      "loss: 0.025228  [17984/31287]\n",
      "loss: 0.001399  [18048/31287]\n",
      "loss: 0.005460  [18112/31287]\n",
      "loss: 0.002563  [18176/31287]\n",
      "loss: 0.000581  [18240/31287]\n",
      "loss: 0.204795  [18304/31287]\n",
      "loss: 0.000244  [18368/31287]\n",
      "loss: 0.004944  [18432/31287]\n",
      "loss: 0.000035  [18496/31287]\n",
      "loss: 0.000447  [18560/31287]\n",
      "loss: 0.000898  [18624/31287]\n",
      "loss: 0.000789  [18688/31287]\n",
      "loss: 0.000246  [18752/31287]\n",
      "loss: 0.113669  [18816/31287]\n",
      "loss: 0.001207  [18880/31287]\n",
      "loss: 0.017099  [18944/31287]\n",
      "loss: 0.000916  [19008/31287]\n",
      "loss: 0.059043  [19072/31287]\n",
      "loss: 0.002926  [19136/31287]\n",
      "loss: 0.018843  [19200/31287]\n",
      "loss: 0.005574  [19264/31287]\n",
      "loss: 0.004849  [19328/31287]\n",
      "loss: 0.012728  [19392/31287]\n",
      "loss: 0.000045  [19456/31287]\n",
      "loss: 0.001666  [19520/31287]\n",
      "loss: 0.001582  [19584/31287]\n",
      "loss: 0.000110  [19648/31287]\n",
      "loss: 0.010848  [19712/31287]\n",
      "loss: 0.005533  [19776/31287]\n",
      "loss: 0.014870  [19840/31287]\n",
      "loss: 0.042293  [19904/31287]\n",
      "loss: 0.000339  [19968/31287]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13268\\2925518221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest_loop2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13268\\1534794208.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 state_steps)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             adam(\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop2(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73089f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loop2(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4908ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def test_loop2(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, z in dataloader:\n",
    "            output = model(x, y)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            correct += (predicted == z).sum().item()\n",
    "            \n",
    "            y_true.extend(z.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    accuracy = correct / size\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e7bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
